{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### flume的特点\n",
    "* 数据源可定制\n",
    "* 数据存储可定制\n",
    "* 保证数据不丢失\n",
    "\n",
    "#### flume核心概念\n",
    "* Event: 日志\n",
    "* Client: 产生日志的服务器\n",
    "* Agent: flume (本质是生产者,消费者问题Source生产数据,Sink消费数据)\n",
    "    * Source: 与数据源连接的组件\n",
    "    * Sink: 与数据目的地连接的组件\n",
    "    * Channel: 连接桥梁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source \n",
    "* Source负责接收Event或通过机制产生Event\n",
    "* hdfs多个小文件与一个大文件的劣势\n",
    "    * 小文件对应Block的数量太多, 文件到Block的映射很多, namenode内存记录的压力很大.   \n",
    "      但是block对应的datanode节点不需要存在memory中. 因为hdfs在启动时, datanode要向namenode汇报自己的block数, 此时namenode再把block与datanode的匹配信息存在内存中\n",
    "    * 小文件数量多, 文件的描述信息也会很多: 文件目录树,修改时间,文件类型等. 这也在namenode的memory中\n",
    "\n",
    "#### 数据格式的演化\n",
    "* 第一代 csv,tsv: 直观简单,但不知道每列的含义\n",
    "* 第二代json,xml: 可自我解释的半结构化数据, 但不知道数据类型. (比如5,不知道是int还是long)\n",
    "* 第三代protobuf: \n",
    "    * 带全量的schema: 变量名和变量类型\n",
    "    * 数据高压缩: 解决IO瓶颈\n",
    "    * 可对写入的数据进行类型校验: 如果定为int的变量写成string, 会报错"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spool dir与tail dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
