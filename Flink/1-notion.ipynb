{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$1. Introduction\n",
    "#### What is Apache Flink?\n",
    "* Flink是工作在无界(Unbounded streams)和有界数据流(Bounded streams)上的计算引擎  \n",
    "* 所有种类的数据, 都能被看做由事件流(stream of events)产生的: 像信用卡交易,机器日志,用户在手机和网页上的交互动作等\n",
    "* Unbounded streams: \n",
    "    * 无界流数据要被迅速处理, 不等企图等待所有数据到达以后再处理, 因为无界数据流不会停止, 有开始, 无结束\n",
    "    * 处理无界数据需要事件(event)以某种顺序抵达\n",
    "* Bounded streams:\n",
    "    * 指通常的批处理, 有开始和结束\n",
    "    * 有界数据流不需要事件按顺序到达, 因为有界数据可能已经被排序\n",
    "* Flink精通于同时处理有界和无界数据. Flink对时间和状态的精确控制, 使得其可在无界数据流上运行各种应用;  \n",
    "  同时专门为有界数据流设计了数据结构和算法来加速处理\n",
    "  \n",
    "#### Run Applications at any Scale\n",
    "* Flink的异步性, 和持续增加的chaeck point机制, 会使得处理延迟带来的影响到最小, 保证处理数据是exactly-once一致性\n",
    "\n",
    "#### Building Blocks for Streaming Applications\n",
    "流处理框架主要看如何处理streams, state, time; 下面依次介绍\n",
    "* Streams\n",
    "    * Bounded and unbounded streams: Flink可以处理有界和无界数据\n",
    "    * Real-time and recorded streams: Flink可以对数据进行实时处理和先记录再处理\n",
    "* State\n",
    "* Time\n",
    "    * 重要的是, 应用如何处理event-time和processing-time\n",
    "    * Event-time Mode:\n",
    "    * Watermark Support:\n",
    "    * Late Data Handling:\n",
    "    * Processing-time Mode:\n",
    "    \n",
    "#### DeployMode\n",
    "* **Start a Local Flink Cluster**  \n",
    "\n",
    "```bash \n",
    "$ ./bin/start-cluster.sh  # Start Flink\n",
    "$ ./bin/stop-cluster.sh   # Stop Flink\n",
    "```\n",
    "\n",
    "* **Create YARN Session**  \n",
    " YARN session是在Hadoop YARN环境下启动一个Flink cluster集群，里面的资源是可以共享给其他的Flink作业。 \n",
    "```bash  \n",
    "$ ./bin/yarn-session.sh -n 4 -tm 8192 -s 8\n",
    " # 上面命令启动了4个TaskManager，每个TaskManager内存为8G且占用了8个核\n",
    "```\n",
    "\n",
    "```bash\n",
    "$ ./bin/flink run ./examples/batch/WordCount.jar\n",
    "```\n",
    "\n",
    "* **Run a single Flink job on YARN**   \n",
    " 我们还可以在YARN上启动一个Flink作业。这里我们还是使用./bin/flink，但是不需要事先启动YARN session：\n",
    "```shell\n",
    "$ ./bin/flink run -m yarn-cluster -yn 2 ./examples/batch/WordCount.jar \n",
    "```\n",
    "\n",
    "* **Scala REPL**\n",
    "```shell\n",
    "bin/start-scala-shell.sh local\n",
    "bin/start-scala-shell.sh yarn -n 2 # Yarn Scala Shell cluster\n",
    "bin/start-scala-shell.sh yarn      # Yarn session\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  $\\S$2.应用场景\n",
    "\n",
    "[https://flink.apache.org/zh/usecases.html](https://flink.apache.org/zh/usecases.html)\n",
    "* 事件驱动型应用\n",
    "* 数据分析应用\n",
    "* 数据管道应用\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  $\\S$3. Concepts\n",
    "#### Dataflow Programming Model\n",
    "* **Programs and Dataflows**\n",
    "    * Flink程序都会被映射成streaming dataflows,由streams和operators组成 (a stream is a (potentially never-ending) flow of data records);   \n",
    "    输入称为connector,输出称为sink\n",
    "    * Parallel Dataflows\n",
    "        * Flink的operator分为两类(对比spark的款依赖, 窄依赖)\n",
    "            * One-to-one streams\n",
    "            * Redistributing streams \n",
    "    * [parallel execution](https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/parallel.html)  \n",
    "* **Windows(窗口)**    \n",
    "    * 流处理的Aggregating操作与批处理不同, 因为流处理没有bound, 所以要指定流处理的window, 作为聚合操作的作用范围\n",
    "    * window可以分为两种: 面向时间(time)的, 也可以是面向数据(data)的.  \n",
    "      例如: “count over the last 5 minutes”, or “sum of the last 100 elements”\n",
    "    * 从另一个角度讲, window还可以被划分为3种: \n",
    "        * 滚动窗口: tumbling windows (窗口之间无交叉)\n",
    "        * 滑动窗口: sliding windows (窗口之间有交叉)\n",
    "        * session窗口: 被不活动的时间间隔切割成窗口\n",
    "    * ref\n",
    "        * [blog](https://flink.apache.org/news/2015/12/04/Introducing-windows.html)\n",
    "        * [docs](https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/stream/operators/windows.html)\n",
    "* **Time** *\n",
    "    * 当谈及流式处理中time(时间)时, 通常在讨论三种时间\n",
    "        * Event time: event被创建的时间,通常是一个时间戳, 由生产者产生\n",
    "        * Ingestion time: event进入flink dataflow的时间\n",
    "        * Processing time: 每个flink节点处理数据的本地时间\n",
    "    * [doc](https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/event_time.html)\n",
    "* **Stateful Operations** \n",
    "    * Flink中的许多操作都是针对单个事件的, 比如event parser. 但有些操作需要跨多个事件, 例如window operators, 这种操作称为stateful\n",
    "    * stateful operations的状态可以被认为是存在一个嵌入式的key/value存储里, 这些状态会被重新分区并分不到多台机器中.  \n",
    "      使用`keyBy()`方法后, 会根据事件的key重新分布事件, 让后续的更新操作为本地操作\n",
    "* **Checkpoints实现容错**\n",
    "    * flink使用流回放(streaming replay)和检查点(checkpoint)实现容错\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  $\\S$4. Distributed Runtime Environment\n",
    "\n",
    "* **Job Managers, Task Managers, Clients**\n",
    "    * Job Managers: mster的角色, 负责调度任务, 协调checkpoint等\n",
    "    * Task Managers: worder角色, 负责执行任务, buffer和exchange数据流\n",
    "    * Clients: client不是flink运行时的一部分,只负责准备和发送数据流到Job Manager, 此后就可以断开与flink的连接,也可以持续保持连接等待报告返回\n",
    "* **Task Slots and Resources**\n",
    "    * task slots: \n",
    "        * 每个worker都是一个jvm进程, 可以启用1个或多个线程执行task.\n",
    "        * slot数量表示每个worker启动几个线程处理子任务, 默认每个线程处理一个子任务, 每个线程均分worker占用的内存, worker至少有一个slot\n",
    "* **State Backends**\n",
    "    *  key/values的存储形式取决于flink选用的state后端. 可选择的后端有内存中的hash-map结构, 和嵌入式kv数据库RocksDB\n",
    "    * state后端除了负责存储key/values, 同时也负责对key/values左check-point\n",
    "    \n",
    "* **Savepoints**\n",
    "    * Savepoints是手动触发的checkpoint, 可以让datastream下次从该位置继续执行\n",
    "    * Savepoints与checkpoint的区别是: Savepoints需要手动触发, 且不会自动失效(当新的checkpoint产生后)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
