{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. RDD basic\n",
    "reduce把任务发送到不同机器上执行. 这些机器各自持有map结果的一部分, 并在这部分上进行local reduce, 最后将结果发送给driver programme机器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines = data.txt MapPartitionsRDD[12] at textFile at <console>:30\n",
       "lineLengths = MapPartitionsRDD[13] at map at <console>:31\n",
       "totalLength = 7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[13] at map at <console>:31"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"data.txt\")\n",
    "val lineLengths = lines.map(s => s.length)  //懒计算\n",
    "val totalLength = lineLengths.reduce((a, b) => a + b)  // action操作,触发计算\n",
    "lineLengths.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 闭包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter value: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "counter = 0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var counter = 0\n",
    "// Wrong: Don't do this!!\n",
    "lineLengths.foreach(x => {counter += x})\n",
    "println(\"Counter value: \" + counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 如上操作不会正常执行:  \n",
    " 1. spark将rdd操作打散成多个`task`, 每个task由`executor`执行.   \n",
    " 2. 在任务被执行前, spark讲rdd的闭包序列化后发送给每个`executor`, (此处是foreach). 闭包中的变量, 都是复制到`executor`所在机器上的, 每个`executor`操作的也是本地机器上的变量, 而不是driver programme机器上的变量. 因此, 在上述程序输出sounter时, 仍然输出的是driver机器内存里的counter(等于0), 而不是`executor`计算rdd后的输出结果\n",
    " \n",
    "2. Accumulator  \n",
    "像如上这种, 需要在集群中享变量进行和操作的情况, 需要使用`Accumulator`  \n",
    "下面, 我们定义一个long型的Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accum = LongAccumulator(id: 302, name: Some(my long accumlater), value: 15)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accum = sc.longAccumulator(\"my long accumlater\")\n",
    "sc.parallelize(Array(1,2,4,8)).foreach(x=>accum.add(x))\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Printing elements of an RDD\n",
    "1. 闭包中的print只在`executor`上执行, 而不会显现在driver机器上.  \n",
    " 例如: `rdd.foreach(println) or rdd.map(println)`  \n",
    "2. `rdd.collect().foreach(println)`会导致driver机器内存溢出. 因为`collect()`会收集所有executor产生的数据  \n",
    "3. 少量数据打印可使用`rdd.take(100).foreach(println)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Working with Key-Value Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
