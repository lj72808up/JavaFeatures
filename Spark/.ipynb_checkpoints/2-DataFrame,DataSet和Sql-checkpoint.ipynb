{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 DataFrame\n",
    "#### 1. Creating DataFrames\n",
    "1. `Dataset`:  \n",
    " `Datasets`是一种分布式数据集, 拥有RDD的优势(强类型,支持lambda表达式).一个 Dataset 可以从JVM对象来构造并且使用transformation算子\n",
    "2. `DataFrame`  \n",
    "  `DataFrame`在scala api中仅仅是`Dataset[ROW]`类型的别名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.sql.Dataset\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|null|Michael|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [age: bigint, name: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: bigint, name: string]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// org.apache.spark.sql.Dataset\n",
    "val df = spark.read.json(\"people.json\")\n",
    "println(df.getClass)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 无类型的Dataset操作 (DataFrame)\n",
    "1. 因为Dataset是一组java对象组成的, 这些对象是强类型的, 其设计思路与RDD一致.  \n",
    "2. DataFrame概念上被设计成无类型的Dataset,即多个\"Row\"对象组成的Dataset, 即DataFrame=Dataset[Row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Print the schema in a tree format\n",
    "df.printSchema()\n",
    "// root\n",
    "// |-- age: long (nullable = true)\n",
    "// |-- name: string (nullable = true)\n",
    "\n",
    "// Select only the \"name\" column\n",
    "df.select(\"name\").show()\n",
    "// +-------+\n",
    "// |   name|\n",
    "// +-------+\n",
    "// |Michael|\n",
    "// |   Andy|\n",
    "// | Justin|\n",
    "// +-------+\n",
    "\n",
    "// Select everybody, but increment the age by 1\n",
    "df.select($\"name\", $\"age\" + 1).show()\n",
    "// +-------+---------+\n",
    "// |   name|(age + 1)|\n",
    "// +-------+---------+\n",
    "// |Michael|     null|\n",
    "// |   Andy|       31|\n",
    "// | Justin|       20|\n",
    "// +-------+---------+\n",
    "\n",
    "// Select people older than 21\n",
    "df.filter($\"age\" > 21).show()\n",
    "// +---+----+\n",
    "// |age|name|\n",
    "// +---+----+\n",
    "// | 30|Andy|\n",
    "// +---+----+\n",
    "\n",
    "// Count people by age\n",
    "df.groupBy(\"age\").count().show()\n",
    "// +----+-----+\n",
    "// | age|count|\n",
    "// +----+-----+\n",
    "// |  19|    1|\n",
    "// |null|    1|\n",
    "// |  30|    1|\n",
    "// +----+-----+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Running SQL Queries on DataFrames\n",
    "\n",
    "1. SparkSession 的 sql 函数可以让应用程序以编程的方式运行 SQL 查询, 并将结果作为一个 DataFrame 返回."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|null|Michael|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sqlDF = [age: bigint, name: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: bigint, name: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "val sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 全局临时视图  \n",
    "Spark SQL中的临时视图是session级别的, 也就是会随着session的消失而消失. 如果你想让一个临时视图在所有session中相互传递并且可用, 直到Spark 应用退出, 你可以建立一个全局的临时视图.全局的临时视图存在于系统数据库 global_temp中, 我们必须加上库名去引用它, 比如. SELECT * FROM global_temp.view1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Register the DataFrame as a global temporary view\n",
    "df.createGlobalTempView(\"people\")\n",
    "\n",
    "// Global temporary view is tied to a system preserved database `global_temp`\n",
    "spark.sql(\"SELECT * FROM global_temp.people\").show()\n",
    "// +----+-------+\n",
    "// | age|   name|\n",
    "// +----+-------+\n",
    "// |null|Michael|\n",
    "// |  30|   Andy|\n",
    "// |  19| Justin|\n",
    "// +----+-------+\n",
    "\n",
    "// Global temporary view is cross-session\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()\n",
    "// +----+-------+\n",
    "// | age|   name|\n",
    "// +----+-------+\n",
    "// |null|Michael|\n",
    "// |  30|   Andy|\n",
    "// |  19| Justin|\n",
    "// +----+-------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Datasets\n",
    "#### 1. Creating Datasets\n",
    "1. Datasets和RDD相似, 但是Datasets没有使用java序列化或`Kryo`序列化. 它使用`Encoder`将对象序列化称bytes.  \n",
    " `encoders`可以动态产生代码, 在集群网络中传递, 而且Spark不需要反序列化就能在这些对象上执行filter,sort等操作  \n",
    "2. Dataset可由`Seq`生成, 也可从DataFrame转化而来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val myspark = spark\n",
    "import myspark.implicits._\n",
    "\n",
    "// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,\n",
    "// you can use custom classes that implement the Product interface\n",
    "// case class Person(name: Option[String], age: Option[Long])\n",
    "case class Person(name: String, age: Long)\n",
    "\n",
    "// Encoders are created for case classes\n",
    "val caseClassDS = Seq(Person(\"Andy\", 32)).toDS()\n",
    "caseClassDS.show()\n",
    "// +----+---+\n",
    "// |name|age|\n",
    "// +----+---+\n",
    "// |Andy| 32|\n",
    "// +----+---+\n",
    "\n",
    "\n",
    "// Encoders for most common types are automatically provided by importing spark.implicits._\n",
    "val primativeDS = Seq(1,2,3).toDS\n",
    "primativeDS.map(x=>x+1).collect // Returns: Array(2, 3, 4)\n",
    "\n",
    "\n",
    "// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name\n",
    "val path = \"people.json\"\n",
    "val peopleDS = spark.read.json(path).as[Person]\n",
    "peopleDS.collect\n",
    "// res4: Array[Person] = Array(Person(Some(Michael),None), \n",
    "//                             Person(Some(Andy),Some(30)), \n",
    "//                             Person(Some(Justin),Some(19))\n",
    "//                            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 RDD互操作  \n",
    "Spark SQL 支持两种不同的方法用于转换已存在的RDD成为Dataset :  \n",
    "1. 第一种方法是使用反射去推断一个包含指定的对象类型的 RDD 的 Schema   \n",
    "2. 第二种用于创建 Dataset 的方法是通过一个允许你构造一个 Schema 然后把它应用到一个已存在的 RDD 的编程接口.    \n",
    "\n",
    "#### 1. 利用反射推断schama  \n",
    "1. Spark SQL的Scala接口支持自动转换一个包含 case classes的RDD为DataFrame.  \n",
    "2. Case class定义了表的Schema.Case class的参数名使用反射读取并且成为了列名.  \n",
    "3. Case class也可以是嵌套的或者包含像Seq或者Array这样的复杂类型.这个RDD能够被隐式转换成一个DataFrame然后被注册为一个表.表可以用于后续的SQL语句."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val myspark = spark\n",
    "import myspark.implicits._\n",
    "\n",
    "case class Person(name: String, age: Long)\n",
    "\n",
    "val rdd1 = spark.sparkContext.textFile(\"people.txt\")\n",
    "val rdd2 = rdd1.map(x=>x.split(\",\")).map(x=>Person(x(0),x(1).trim.toInt)) // Rdd[Person]  \n",
    "val peopleDF = rdd2.toDF\n",
    "peopleDF.collect\n",
    "println()\n",
    "// [Michael,29]\n",
    "// [Andy,30]\n",
    "// [Justin,19]\n",
    "\n",
    "\n",
    "// Register the DataFrame as a temporary view\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "// SQL statements can be run by using the sql methods provided by Spark\n",
    "val teenagersDF = spark.sql(\"SELECT name, age FROM people WHERE age BETWEEN 13 AND 19\")\n",
    "\n",
    "// The columns of a row in the result can be accessed by field index\n",
    "val rdd1 = teenagersDF.map(row =>\"Name:\"+row(0))\n",
    "println(rdd1.collect)\n",
    "// or by field name\n",
    "val rdd2 - teenagersDF.map(row=>\"Name:\"+row.getAs[String](\"name\"))\n",
    "println(rdd2.collect)\n",
    "\n",
    "// No pre-defined encoders for Dataset[Map[K,V]], define explicitly\n",
    "implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String,Any]]\n",
    "val rdd3 = teenagersDF.map(row=>row.getValuesMap[Any](List(\"name\",\"age\")))\n",
    "println(rdd3.coolect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 构造StructType对象\n",
    "1. 从原始RDD中创建RDD[ROW]  \n",
    "2. 创建StructType匹配RDD中的Row  \n",
    "3. 通过 SparkSession 提供的 createDataFrame 方法应用 Schema 到 RDD 的 RowS（行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([WrappedArray(29, 30, 19)])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Row\n",
    "val path = \"/Users/lj/devkits/spark-2.3.0-bin-hadoop2.7/examples/src/main/resources/people.txt\"\n",
    "// StructType\n",
    "val schema = StructType(Array(StructField(\"name\",StringType,nullable=true),\n",
    "                             StructField(\"age\",StringType,nullable=true)))\n",
    "// Origion RDD\n",
    "val rdd1 = spark.sparkContext.textFile(path)\n",
    "val rdd2 = rdd1.map(x=>x.split(\",\"))\n",
    "val rdd3 = rdd2.map(x=>Row(x(0),x(1).trim))  // RDD[Row]\n",
    "\n",
    "//DataFrame\n",
    "val peopleDF = spark.createDataFrame(rdd3,schema)  // RDD to Dataframe\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "val res = spark.sql(\"select collect_list(age) from people\")\n",
    "res.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
