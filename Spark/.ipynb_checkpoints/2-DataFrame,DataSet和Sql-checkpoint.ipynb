{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 DataFrame\n",
    "#### 1. Creating DataFrames\n",
    "1. `Dataset`:  \n",
    " `Datasets`是一种分布式数据集, 拥有RDD的优势(强类型,支持lambda表达式).  \n",
    " `Datasets`支持各种函数式算子  \n",
    "2. `DataFrame`  \n",
    "  `DataFrame`和python中的DataFrame在概念上一样. 是`Dataset+列名`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [age: bigint, name: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: bigint, name: string]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// org.apache.spark.sql.Dataset\n",
    "val df = spark.read.json(\"people.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.sql.Dataset"
     ]
    }
   ],
   "source": [
    "print(df.getClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select($\"name\",$\"age\"+1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter($\"age\">21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy($\"age\").count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:30: error: not found: value collect_list\n",
       "       df.agg(collect_list($\"name\"), collect_set($\"name\"))\n",
       "              ^\n",
       "<console>:30: error: not found: value collect_set\n",
       "       df.agg(collect_list($\"name\"), collect_set($\"name\"))\n",
       "                                     ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.agg(collect_list($\"name\"), collect_set($\"name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Running SQL Queries on DataFrames\n",
    "1. 注册DataFrame为全局试图  \n",
    "2. 执行`sql`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Register the DataFrame as a global temporary view\n",
    "df.createGlobalTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.sql.Dataset\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res = [age: bigint, name: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: bigint, name: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val res = spark.sql(\"select * from global_temp.people\")\n",
    "println(res.getClass)\n",
    "res.show()\n",
    "\n",
    "// Global temporary view is cross-session\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Datasets\n",
    "#### 1. Creating Datasets\n",
    "1. Datasets和RDD相似, 但是Datasets没有使用java序列化或`Kryo`序列化. 它使用`Encoder`将对象序列化称bytes.  \n",
    " `encoders`可以动态产生代码, 在集群网络中传递, 而且Spark不需要反序列化就能在这些对象上执行filter,sort等操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Person\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Person(name:Option[String],age:Option[Long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|zhangsan| 25|\n",
      "+--------+---+\n",
      "\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "myspark = org.apache.spark.sql.SparkSession@795e04fc\n",
       "caseClassDS = [name: string, age: bigint]\n",
       "primitiveDS = [value: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: int]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myspark = spark\n",
    "import myspark.implicits._   // 通配符_需要从val变量中使用, 而环境中内置的spark对象是var的\n",
    "\n",
    "// Encoders are created for case classes\n",
    "val caseClassDS = Seq(Person(Some(\"zhangsan\"),Some(25))).toDS\n",
    "caseClassDS.show()\n",
    "\n",
    "// Encoders for most common types are automatically provided by importing spark.implicits._\n",
    "val primitiveDS = Seq(1, 2, 3).toDS()\n",
    "primitiveDS.map(_ + 1).collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `DataSet`可以从`DataFrame`转换而来, 通过指定转换的`class`  \n",
    " (要求class的属性名和DataFrame的column name匹配)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person(Some(Michael),None)\n",
      "Person(Some(Andy),Some(30))\n",
      "Person(Some(Justin),Some(19))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ds = [age: bigint, name: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: bigint, name: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name\n",
    "val ds = spark.read.json(\"people.json\").as[Person]\n",
    "ds.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用case class反射推断DataFrame为DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [age: bigint, name: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: bigint, name: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import myspark.implicits._\n",
    "val df = spark.read.json(\"people.json\")\n",
    "df.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|  Andy| 30|\n",
      "|Justin| 19|\n",
      "+------+---+\n",
      "\n",
      "name: Andy\n",
      "name: Justin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "teenagerDF = [name: string, age: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[name: string, age: bigint]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val teenagerDF = spark.sql(\"SELECT name, age FROM people WHERE age BETWEEN 12 AND 31\")\n",
    "teenagerDF.show\n",
    "// The columns of a row in the result can be accessed by field index\n",
    "teenagerDF.map(x=>\"name: \"+x(0)).collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andy\n",
      "Justin\n"
     ]
    }
   ],
   "source": [
    "// or by field name\n",
    "teenagerDF.map(x=>x.getAs[String](\"name\")).collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map(name -> Andy, age -> 30)\n",
      "Map(name -> Justin, age -> 19)\n"
     ]
    }
   ],
   "source": [
    "// 对于未在spark.implicit._中出现的encoder, 需要显式声明\n",
    "implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String,Any]]  // 任意类型转换为String\n",
    "// val ds = teenagerDF.map(row => row.getValuesMap[Any](List(\"age\",\"name\")))\n",
    "// ds.collect.foreach(println)\n",
    "\n",
    "teenagerDF.map(_.getValuesMap[Any](List(\"name\", \"age\"))).collect().foreach(println)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
