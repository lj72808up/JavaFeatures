{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 基本统计\n",
    "\n",
    "#### 一. 相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(4,[0,3],[1.0,-2.0])|\n",
      "|   [4.0,5.0,0.0,3.0]|\n",
      "|   [6.0,7.0,0.0,8.0]|\n",
      "| (4,[0,3],[9.0,1.0])|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val myspark = spark\n",
    "import myspark.implicits._\n",
    "import org.apache.spark.ml.linalg.{Matrix, Vectors}\n",
    "import org.apache.spark.ml.stat.Correlation\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val data = Seq(\n",
    "    Vectors.sparse(4,Seq((0,1.0),(3,-2.0))),\n",
    "    Vectors.dense(4.0,5.0,0.0,3.0),\n",
    "    Vectors.dense(6.0,7.0,0.0,8.0),\n",
    "    Vectors.sparse(4,Seq((0,9.0),(3,1.0)))\n",
    ")\n",
    "val df = data.map(Tuple1.apply).toDF(\"features\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      " 1.0                   0.055641488407465814  NaN  0.4004714203168137  \n",
      "0.055641488407465814  1.0                   NaN  0.9135958615342522  \n",
      "NaN                   NaN                   1.0  NaN                 \n",
      "0.4004714203168137    0.9135958615342522    NaN  1.0                 \n",
      "Spearman correlation matrix:\n",
      " 1.0                  0.10540925533894532  NaN  0.40000000000000174  \n",
      "0.10540925533894532  1.0                  NaN  0.9486832980505141   \n",
      "NaN                  NaN                  1.0  NaN                  \n",
      "0.40000000000000174  0.9486832980505141   NaN  1.0                  \n"
     ]
    }
   ],
   "source": [
    "val Row(coeff1:Matrix) = Correlation.corr(df,\"features\").head\n",
    "val Row(coeff2:Matrix) = Correlation.corr(df,\"features\",\"spearman\").head\n",
    "\n",
    "println(s\"Pearson correlation matrix:\\n $coeff1\")\n",
    "println(s\"Spearman correlation matrix:\\n $coeff2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 二. 卡方检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pValues = [0.6872892787909721,0.44089552967916945]\n",
      "degreesOfFreedom [2,4]\n",
      "statistics [0.75,3.7500000000000004]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.stat.ChiSquareTest\n",
    "\n",
    "val data = Seq(\n",
    "  (0.0, Vectors.dense(0.5, 10.0)),\n",
    "  (0.0, Vectors.dense(1.5, 20.0)),\n",
    "  (1.0, Vectors.dense(1.5, 30.0)),\n",
    "  (0.0, Vectors.dense(3.5, 30.0)),\n",
    "  (0.0, Vectors.dense(3.5, 40.0)),\n",
    "  (1.0, Vectors.dense(3.5, -40.0))\n",
    ")\n",
    "val df = data.toDF(\"label\",\"features\")\n",
    "val chi = ChiSquareTest.test(df,\"features\",\"label\").head\n",
    "\n",
    "println(s\"pValues = ${chi.getAs[Vector](0)}\")\n",
    "println(s\"degreesOfFreedom ${chi.getSeq[Int](1).mkString(\"[\", \",\", \"]\")}\")\n",
    "println(s\"statistics ${chi.getAs[Vector](2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 PipeLine\n",
    "\n",
    "#### 一. Example: Estimator, Transformer, and Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val data = Seq(\n",
    "  (1.0, Vectors.dense(0.0, 1.1, 0.1)),\n",
    "  (0.0, Vectors.dense(2.0, 1.0, -1.0)),\n",
    "  (0.0, Vectors.dense(2.0, 1.3, 1.0)),\n",
    "  (1.0, Vectors.dense(0.0, 1.2, -0.5))\n",
    ").toDF(\"label\", \"features\")\n",
    "\n",
    "val clf = new LogisticRegression()\n",
    "// println(s\"LogisticRegression parameters:\\n ${clf.explainParams()}\")\n",
    "println(s\"LogisticRegression parameters:\\n ${clf.explainParams()}\\n\")\n",
    "\n",
    "clf.setMaxIter(10).\n",
    "    setRegParam(0.01)\n",
    "\n",
    "val model1 = clf.fit(data)\n",
    "// This prints the parameter (name: value) pairs, where names are unique IDs for this LogisticRegression instance.\n",
    "println(s\"Model 1 was fit using parameters: ${model1.parent.extractParamMap()}\")\n",
    "\n",
    "//使用ParamMap指定参数\n",
    "val paramMap = ParamMap(clf.maxIter->20).\n",
    "                    put(clf.maxIter->30).  // Specify 1 Param. This overwrites the original maxIter.\n",
    "                    put(clf.regParam->0.1, clf.threshold->0.55)  //// Specify multiple Params.\n",
    "\n",
    "//组合多个ParamMap\n",
    "val paramMap2 = ParamMap(clf.probabilityCol->\"myProbility\")\n",
    "val paramMapCombine = paramMap ++ paramMap2\n",
    "\n",
    "// clf.fit时指定的参数会覆盖声明clf时指定的参数\n",
    "val model2 = clf.fit(data,paramMapCombine\n",
    "println(s\"Model 2 was fit using parameters: ${model2.parent.extractParamMap}\")\n",
    "\n",
    "\n",
    "// Prepare test data.\n",
    "val test_data = spark.createDataFrame(Seq(\n",
    "  (1.0, Vectors.dense(-1.0, 1.5, 1.3)),\n",
    "  (0.0, Vectors.dense(3.0, 2.0, -0.1)),\n",
    "  (1.0, Vectors.dense(0.0, 2.2, -1.5))\n",
    ")).toDF(\"label\", \"features\")\n",
    "// Make predictions on test data using the Transformer.transform() method.\n",
    "// LogisticRegression.transform will only use the 'features' column.\n",
    "// Note that model2.transform() outputs a 'myProbability' column instead of the usual\n",
    "// 'probability' column since we renamed the lr.probabilityCol parameter previously.\n",
    "model2.transform(test_data).\n",
    "    select(\"features\",\"label\",\"myProbility\",\"prediction\").\n",
    "    collect().\n",
    "    foreach(\n",
    "        case Row(features:Vector,label:Double,prob:Vector,prediction:Double) => \n",
    "            println(s\"($features, $label) -> prob=$prob, prediction=$prediction\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 二. Example: Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data = [id: bigint, text: string ... 1 more field]\n",
       "tokenizer = tok_c0b3bee4beb2\n",
       "hashingTF = hashingTF_58319b374a93\n",
       "lr = logreg_c88c60484ad2\n",
       "pipeline = pipeline_626438889f4a\n",
       "model = pipeline_626438889f4a\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_626438889f4a"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val data = Seq(\n",
    "  (0L, \"a b c d e spark\", 1.0),\n",
    "  (1L, \"b d\", 0.0),\n",
    "  (2L, \"spark f g h\", 1.0),\n",
    "  (3L, \"hadoop mapreduce\", 0.0)\n",
    ").toDF(\"id\", \"text\", \"label\")\n",
    "// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "val tokenizer = new Tokenizer().setInputCol(\"text\").setOutputCol(\"word\")\n",
    "val hashingTF = new HashingTF().setNumFeatures(1000).setInputCol(tokenizer.getOutputCol).setOutputCol(\"features\")\n",
    "val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.001)\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer,hashingTF,lr))\n",
    "val model = pipeline.fit(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
