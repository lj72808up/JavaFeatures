{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 DataFrame\n",
    "#### 1. Creating DataFrames\n",
    "1. `Dataset`:  \n",
    " `Datasets`是一种分布式数据集, 拥有RDD的优势(强类型,支持lambda表达式).一个 Dataset 可以从JVM对象来构造并且使用transformation算子\n",
    "2. `DataFrame`  \n",
    "  `DataFrame`在scala api中仅仅是`Dataset[ROW]`类型的别名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.sql.Dataset\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|null|Michael|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [age: bigint, name: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: bigint, name: string]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// org.apache.spark.sql.Dataset\n",
    "val df = spark.read.json(\"people.json\")\n",
    "println(df.getClass)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 无类型的Dataset操作 (DataFrame)\n",
    "1. 因为Dataset是一组java对象组成的, 这些对象是强类型的, 其设计思路与RDD一致.  \n",
    "2. DataFrame概念上被设计成无类型的Dataset,即多个\"Row\"对象组成的Dataset, 即DataFrame=Dataset[Row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Print the schema in a tree format\n",
    "df.printSchema()\n",
    "// root\n",
    "// |-- age: long (nullable = true)\n",
    "// |-- name: string (nullable = true)\n",
    "\n",
    "// Select only the \"name\" column\n",
    "df.select(\"name\").show()\n",
    "// +-------+\n",
    "// |   name|\n",
    "// +-------+\n",
    "// |Michael|\n",
    "// |   Andy|\n",
    "// | Justin|\n",
    "// +-------+\n",
    "\n",
    "// Select everybody, but increment the age by 1\n",
    "df.select($\"name\", $\"age\" + 1).show()\n",
    "// +-------+---------+\n",
    "// |   name|(age + 1)|\n",
    "// +-------+---------+\n",
    "// |Michael|     null|\n",
    "// |   Andy|       31|\n",
    "// | Justin|       20|\n",
    "// +-------+---------+\n",
    "\n",
    "// Select people older than 21\n",
    "df.filter($\"age\" > 21).show()\n",
    "// +---+----+\n",
    "// |age|name|\n",
    "// +---+----+\n",
    "// | 30|Andy|\n",
    "// +---+----+\n",
    "\n",
    "// Count people by age\n",
    "df.groupBy(\"age\").count().show()\n",
    "// +----+-----+\n",
    "// | age|count|\n",
    "// +----+-----+\n",
    "// |  19|    1|\n",
    "// |null|    1|\n",
    "// |  30|    1|\n",
    "// +----+-----+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Running SQL Queries on DataFrames\n",
    "\n",
    "1. SparkSession 的 sql 函数可以让应用程序以编程的方式运行 SQL 查询, 并将结果作为一个 DataFrame 返回."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|null|Michael|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sqlDF = [age: bigint, name: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: bigint, name: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "val sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 全局临时视图  \n",
    "Spark SQL中的临时视图是session级别的, 也就是会随着session的消失而消失. 如果你想让一个临时视图在所有session中相互传递并且可用, 直到Spark 应用退出, 你可以建立一个全局的临时视图.全局的临时视图存在于系统数据库 global_temp中, 我们必须加上库名去引用它, 比如. SELECT * FROM global_temp.view1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Register the DataFrame as a global temporary view\n",
    "df.createGlobalTempView(\"people\")\n",
    "\n",
    "// Global temporary view is tied to a system preserved database `global_temp`\n",
    "spark.sql(\"SELECT * FROM global_temp.people\").show()\n",
    "// +----+-------+\n",
    "// | age|   name|\n",
    "// +----+-------+\n",
    "// |null|Michael|\n",
    "// |  30|   Andy|\n",
    "// |  19| Justin|\n",
    "// +----+-------+\n",
    "\n",
    "// Global temporary view is cross-session\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()\n",
    "// +----+-------+\n",
    "// | age|   name|\n",
    "// +----+-------+\n",
    "// |null|Michael|\n",
    "// |  30|   Andy|\n",
    "// |  19| Justin|\n",
    "// +----+-------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Datasets\n",
    "#### 1. Creating Datasets\n",
    "1. Datasets和RDD相似, 但是Datasets没有使用java序列化或`Kryo`序列化. 它使用`Encoder`将对象序列化称bytes.  \n",
    " `encoders`可以动态产生代码, 在集群网络中传递, 而且Spark不需要反序列化就能在这些对象上执行filter,sort等操作  \n",
    "2. Dataset可由`Seq`生成, 也可从DataFrame转化而来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val myspark = spark\n",
    "import myspark.implicits._\n",
    "\n",
    "// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,\n",
    "// you can use custom classes that implement the Product interface\n",
    "// case class Person(name: Option[String], age: Option[Long])\n",
    "case class Person(name: String, age: Long)\n",
    "\n",
    "// Encoders are created for case classes\n",
    "val caseClassDS = Seq(Person(\"Andy\", 32)).toDS()\n",
    "caseClassDS.show()\n",
    "// +----+---+\n",
    "// |name|age|\n",
    "// +----+---+\n",
    "// |Andy| 32|\n",
    "// +----+---+\n",
    "\n",
    "\n",
    "// Encoders for most common types are automatically provided by importing spark.implicits._\n",
    "val primativeDS = Seq(1,2,3).toDS\n",
    "primativeDS.map(x=>x+1).collect // Returns: Array(2, 3, 4)\n",
    "\n",
    "\n",
    "// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name\n",
    "val path = \"people.json\"\n",
    "val peopleDS = spark.read.json(path).as[Person]\n",
    "peopleDS.collect\n",
    "// res4: Array[Person] = Array(Person(Some(Michael),None), \n",
    "//                             Person(Some(Andy),Some(30)), \n",
    "//                             Person(Some(Justin),Some(19))\n",
    "//                            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 RDD互操作  \n",
    "Spark SQL 支持两种不同的方法用于转换已存在的RDD成为Dataset :  \n",
    "1. 第一种方法是使用反射去推断一个包含指定的对象类型的 RDD 的 Schema   \n",
    "2. 第二种用于创建 Dataset 的方法是通过一个允许你构造一个 Schema 然后把它应用到一个已存在的 RDD 的编程接口.    \n",
    "\n",
    "#### 1. 利用反射推断schama  \n",
    "1. Spark SQL的Scala接口支持自动转换一个包含 case classes的RDD为DataFrame.  \n",
    "2. Case class定义了表的Schema.Case class的参数名使用反射读取并且成为了列名.  \n",
    "3. Case class也可以是嵌套的或者包含像Seq或者Array这样的复杂类型.这个RDD能够被隐式转换成一个DataFrame然后被注册为一个表.表可以用于后续的SQL语句."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.rdd.MapPartitionsRDD\n",
      "Person(Michael,29)Person(Andy,30)Person(Justin,19)[Michael,29]\n",
      "[Andy,30]\n",
      "[Justin,19]\n",
      "Name:Justin\n",
      "Name:Justin\n",
      "Map(name -> Justin, age -> 19)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "myspark = org.apache.spark.sql.SparkSession@8488041\n",
       "defined class Person\n",
       "rdd1 = people.txt MapPartitionsRDD[103] at textFile at <console>:33\n",
       "rdd2 = MapPartitionsRDD[105] at map at <console>:34\n",
       "peopleDF = [name: string, age: bigint]\n",
       "teenagersDF = [name: string, age: bigint]\n",
       "rdd_1 = [value: string]\n",
       "rdd_2 = [value: string]\n",
       "mapEncoder = class[value[0]: binary]\n",
       "rdd_3 = [value: binary]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: binary]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myspark = spark\n",
    "import myspark.implicits._\n",
    "\n",
    "case class Person(name: String, age: Long)\n",
    "\n",
    "val rdd1 = spark.sparkContext.textFile(\"people.txt\")\n",
    "val rdd2 = rdd1.map(x=>x.split(\",\")).map(x=>Person(x(0),x(1).trim.toInt)) // Rdd[Person]  \n",
    "println(rdd2.getClass)\n",
    "rdd2.collect.foreach(print)\n",
    "val peopleDF = rdd2.toDF\n",
    "peopleDF.collect.foreach(println)\n",
    "\n",
    "// [Michael,29]\n",
    "// [Andy,30]\n",
    "// [Justin,19]\n",
    "\n",
    "\n",
    "// Register the DataFrame as a temporary view\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "// SQL statements can be run by using the sql methods provided by Spark\n",
    "val teenagersDF = spark.sql(\"SELECT name, age FROM people WHERE age BETWEEN 13 AND 19\")\n",
    "\n",
    "// The columns of a row in the result can be accessed by field index\n",
    "val rdd_1 = teenagersDF.map(row => \"Name:\"+row(0))\n",
    "rdd_1.collect.foreach(println)\n",
    "// Name:Justin\n",
    "// Name:Justin\n",
    "\n",
    "// or by field name\n",
    "val rdd_2 = teenagersDF.map(row => \"Name:\"+row.getAs[String](\"name\"))\n",
    "rdd_2.collect.foreach(println)\n",
    "\n",
    "\n",
    "// No pre-defined encoders for Dataset[Map[K,V]], define explicitly\n",
    "implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String,Any]]\n",
    "val rdd_3 = teenagersDF.map(row=>row.getValuesMap[Any](List(\"name\",\"age\")))\n",
    "rdd_3.collect.foreach(println)\n",
    "//Map(name -> Justin, age -> 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 构造StructType对象\n",
    "1. 从原始RDD中创建RDD[ROW]  \n",
    "2. 创建StructType匹配RDD中的Row  \n",
    "3. 通过 SparkSession 提供的 createDataFrame 方法应用 Schema 到 RDD 的 RowS（行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([WrappedArray(29, 30, 19)])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Row\n",
    "val path = \"/Users/lj/devkits/spark-2.3.0-bin-hadoop2.7/examples/src/main/resources/people.txt\"\n",
    "// StructType\n",
    "val schema = StructType(Array(StructField(\"name\",StringType,nullable=true),\n",
    "                             StructField(\"age\",StringType,nullable=true)))\n",
    "// Origion RDD\n",
    "val rdd1 = spark.sparkContext.textFile(path)\n",
    "val rdd2 = rdd1.map(x=>x.split(\",\"))\n",
    "val rdd3 = rdd2.map(x=>Row(x(0),x(1).trim))  // RDD[Row]\n",
    "\n",
    "//DataFrame\n",
    "val peopleDF = spark.createDataFrame(rdd3,schema)  // RDD to Dataframe\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "val res = spark.sql(\"select collect_list(age) from people\")\n",
    "res.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. pivot\n",
    "1. 构造数据\n",
    "2. 透视的基本写法  \n",
    "```\n",
    "df\n",
    "  .groupBy(grouping_columns)\n",
    "  .pivot(pivot_column, [values]) \n",
    "  .agg(aggregate_expressions)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Item\n",
       "myspark = org.apache.spark.sql.SparkSession@8488041\n",
       "rdd1 = ParallelCollectionRDD[129] at parallelize at <console>:29\n",
       "df = [date: string, program: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><td>2018-01</td><td>项目1</td><td>100</td></tr>\n",
       "<tr><td>2018-01</td><td>项目2</td><td>200</td></tr>\n",
       "<tr><td>2018-01</td><td>项目3</td><td>300</td></tr>\n",
       "<tr><td>2018-02</td><td>项目1</td><td>1000</td></tr>\n",
       "<tr><td>2018-03</td><td>项目x</td><td>999</td></tr>\n",
       "<tr><td>2018-02</td><td>项目2</td><td>2000</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------+-----+------+\n",
       "| 2018-01 | 项目1 | 100  |\n",
       "| 2018-01 | 项目2 | 200  |\n",
       "| 2018-01 | 项目3 | 300  |\n",
       "| 2018-02 | 项目1 | 1000 |\n",
       "| 2018-03 | 项目x | 999  |\n",
       "| 2018-02 | 项目2 | 2000 |\n",
       "+---------+-----+------+"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Item(date:String,program:String,income:Int)\n",
    "val myspark = spark\n",
    "import myspark.implicits._\n",
    "\n",
    "val rdd1 = sc.parallelize(List(Item(\"2018-01\",\"项目1\",100),\n",
    "                              Item(\"2018-01\",\"项目2\",200),\n",
    "                              Item(\"2018-01\",\"项目3\",300),\n",
    "                              Item(\"2018-02\",\"项目1\",1000),\n",
    "                              Item(\"2018-03\",\"项目x\",999),\n",
    "                              Item(\"2018-02\",\"项目2\",2000)))\n",
    "val df = rdd1.toDF\n",
    "df.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><td>2018-03</td><td>NULL</td><td>NULL</td><td>NULL</td><td>999.0</td></tr>\n",
       "<tr><td>2018-02</td><td>1000.0</td><td>2000.0</td><td>NULL</td><td>NULL</td></tr>\n",
       "<tr><td>2018-01</td><td>100.0</td><td>200.0</td><td>300.0</td><td>NULL</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------+--------+--------+-------+-------+\n",
       "| 2018-03 | NULL   | NULL   | NULL  | 999.0 |\n",
       "| 2018-02 | 1000.0 | 2000.0 | NULL  | NULL  |\n",
       "| 2018-01 | 100.0  | 200.0  | 300.0 | NULL  |\n",
       "+---------+--------+--------+-------+-------+"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.avg\n",
    "df.groupBy(\"date\")\n",
    "  .pivot(\"program\")\n",
    "  .agg(avg($\"income\"))\n",
    "  .collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 时间窗口函数\n",
    "df1为用户展示表, df2为用户点击表. 统计每次展示给用户3天内, 用户的点击次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "val myspark = spark\n",
    "import myspark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------+\n",
      "|userId|itemId|    time|\n",
      "+------+------+--------+\n",
      "| user1| item1|2018-5-6|\n",
      "| user1| item2|2018-5-7|\n",
      "| user2| item1|2018-5-4|\n",
      "| user2| item2|2018-5-6|\n",
      "+------+------+--------+\n",
      "\n",
      "+------+------+---------+\n",
      "|userId|itemId|     time|\n",
      "+------+------+---------+\n",
      "| user1| item1| 2018-5-7|\n",
      "| user1| item1|2018-5-10|\n",
      "| user1| item2| 2018-5-9|\n",
      "| user1| item2| 2018-5-7|\n",
      "| user2| item1| 2018-5-5|\n",
      "| user2| item2| 2018-5-7|\n",
      "+------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// 推送表\n",
    "val df1 = Seq((\"user1\",\"item1\",\"2018-5-6\"),\n",
    "             (\"user1\",\"item2\",\"2018-5-7\"),\n",
    "             (\"user2\",\"item1\",\"2018-5-4\"),\n",
    "             (\"user2\",\"item2\",\"2018-5-6\")).toDF(\"userId\",\"itemId\",\"time\")\n",
    "\n",
    "// 点击表\n",
    "val df2 = Seq((\"user1\",\"item1\",\"2018-5-7\"),\n",
    "             (\"user1\",\"item1\",\"2018-5-10\"),\n",
    "             (\"user1\",\"item2\",\"2018-5-9\"),\n",
    "             (\"user1\",\"item2\",\"2018-5-7\"),\n",
    "             (\"user2\",\"item1\",\"2018-5-5\"),\n",
    "             (\"user2\",\"item2\",\"2018-5-7\")).toDF(\"userId\",\"itemId\",\"time\")\n",
    "df1.show\n",
    "df2.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "// 先用to_date将时间转换成日期类型\n",
    "val df1_cast = df1.select($\"userId\",$\"itemId\",to_date($\"time\",\"yyyy-MM-dd\") as \"time_df1\")\n",
    "val df2_cast = df2.select($\"userId\",$\"itemId\",to_date($\"time\",\"yyyy-MM-dd\") as \"time_df2\")\n",
    "\n",
    "df1_cast.createOrReplaceTempView(\"t1\")\n",
    "df2_cast.createOrReplaceTempView(\"t2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------+\n",
      "|userId|itemId|count(1)|\n",
      "+------+------+--------+\n",
      "| user2| item2|       1|\n",
      "| user1| item2|       2|\n",
      "| user2| item1|       1|\n",
      "| user1| item1|       1|\n",
      "+------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT t.userId,t.itemId,count(1) FROM (\n",
    "              SELECT t1.userId,t1.itemId,t1.time_df1,t2.time_df2,datediff(time_df2,time_df1) as delay \n",
    "                  from t1 left join t2 \n",
    "                  on t1.userId=t2.userId and t1.itemId=t2.itemId \n",
    "                  where datediff(time_df2,time_df1) between 0 and 3\n",
    "           )as t group by t.userId,t.itemId,t.time_df1\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+------+------+----------+-------+\n",
      "|userId|itemId|  time_df1|userId|itemId|  time_df2|   diff|\n",
      "+------+------+----------+------+------+----------+-------+\n",
      "| user1| item1|2018-05-06| user1| item1|2018-05-10|-345600|\n",
      "| user1| item1|2018-05-06| user1| item1|2018-05-07| -86400|\n",
      "| user1| item2|2018-05-07| user1| item2|2018-05-07|      0|\n",
      "| user1| item2|2018-05-07| user1| item2|2018-05-09|-172800|\n",
      "| user2| item1|2018-05-04| user2| item1|2018-05-05| -86400|\n",
      "| user2| item2|2018-05-06| user2| item2|2018-05-07| -86400|\n",
      "+------+------+----------+------+------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// 上个例子用date_diff计算日期差距, 这里用unix_timestamp计算日期差距\n",
    "spark.sql(\"select *,unix_timestamp(time_df1)-unix_timestamp(time_df2) as diff \n",
    "           from t1 left join t2 \n",
    "           on t1.userId=t2.userId and t1.itemId=t2.itemId\")\n",
    ".show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 时间窗口函数\n",
    "在我们介绍如何使用time window之前，我们先来准备一份时间序列数据。本文将使用Apple公司从1980年到2016年期间的股票交易信息。股票数据一共有六列，但是这里我们仅关心Date和Close两列，它们分别代表股票交易时间和当天收盘的价格。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------+---------+---------+--------+---------+\n",
      "|               Date|     Open|     High|      Low|    Close|  Volume|Adj Close|\n",
      "+-------------------+---------+---------+---------+---------+--------+---------+\n",
      "|2016-07-11 00:00:00|    96.75|97.650002|96.730003|96.980003|23298900|96.980003|\n",
      "|2016-07-08 00:00:00|96.489998|96.889999|96.050003|    96.68|28855800|    96.68|\n",
      "|2016-07-07 00:00:00|95.699997|     96.5|95.620003|95.940002|24280900|95.940002|\n",
      "|2016-07-06 00:00:00|94.599998|95.660004|94.370003|95.529999|30770700|95.529999|\n",
      "|2016-07-05 00:00:00|95.389999|95.400002|94.459999|95.040001|27257000|95.040001|\n",
      "|2016-07-01 00:00:00|95.489998|96.470001|95.330002|95.889999|25872300|95.889999|\n",
      "|2016-06-30 00:00:00|94.440002|95.769997|94.300003|95.599998|35836400|95.599998|\n",
      "|2016-06-29 00:00:00|93.970001|94.550003|93.629997|94.400002|36531000|94.400002|\n",
      "|2016-06-28 00:00:00|92.900002|93.660004|92.139999|93.589996|40444900|93.589996|\n",
      "|2016-06-27 00:00:00|     93.0|93.050003|     91.5|92.040001|45489600|92.040001|\n",
      "|2016-06-24 00:00:00|92.910004|94.660004|92.650002|93.400002|75311400|93.400002|\n",
      "|2016-06-23 00:00:00|95.940002|96.290001|    95.25|96.099998|32240200|96.099998|\n",
      "|2016-06-22 00:00:00|    96.25|96.889999|95.349998|95.550003|28971100|95.550003|\n",
      "|2016-06-21 00:00:00|94.940002|96.349998|    94.68|95.910004|35229500|95.910004|\n",
      "|2016-06-20 00:00:00|     96.0|    96.57|95.029999|95.099998|33942300|95.099998|\n",
      "|2016-06-17 00:00:00|96.620003|96.650002|95.300003|95.330002|60595000|95.330002|\n",
      "|2016-06-16 00:00:00|96.449997|    97.75|    96.07|97.550003|31236300|97.550003|\n",
      "|2016-06-15 00:00:00|    97.82|98.410004|97.029999|97.139999|29445200|97.139999|\n",
      "|2016-06-14 00:00:00|    97.32|98.480003|    96.75|97.459999|31931900|97.459999|\n",
      "|2016-06-13 00:00:00|98.690002|99.120003|97.099998|97.339996|38020500|97.339996|\n",
      "+-------------------+---------+---------+---------+---------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "val df = spark.read.\n",
    "                option(\"header\",true).\n",
    "                option(\"inferSchema\",\"true\").\n",
    "                csv(\"data/iteblog_apple.csv\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 步骤一：找出2016年的股票交易数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_2016 = df.filter(\"year(Date)=2016\")  //(year是dataframe里的function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 步骤二：计算平均值\n",
    "现在我们需要对每个星期创建一个窗口，这种类型的窗口通常被称为tumbling window，window一般在group by语句中使用。\n",
    "window方法的第一个参数指定了时间所在的列；第二个参数指定了窗口的持续时间(duration)，它的单位可以是seconds、minutes、hours、days或者weeks。创建好窗口之后，我们可以计算平均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "val myspark = spark\n",
    "import myspark.implicits._   // 自动将$\"col\"转换为DataFrame的Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------+\n",
      "|              start|                end|         close_agg|\n",
      "+-------------------+-------------------+------------------+\n",
      "|2015-12-31 08:00:00|2016-01-07 08:00:00|101.30249774999999|\n",
      "|2016-01-07 08:00:00|2016-01-14 08:00:00| 98.47199859999999|\n",
      "|2016-01-14 08:00:00|2016-01-21 08:00:00| 96.72000125000001|\n",
      "|2016-01-21 08:00:00|2016-01-28 08:00:00|        97.6719984|\n",
      "|2016-01-28 08:00:00|2016-02-04 08:00:00|         96.239999|\n",
      "|2016-02-04 08:00:00|2016-02-11 08:00:00| 94.39799819999999|\n",
      "|2016-02-11 08:00:00|2016-02-18 08:00:00|        96.2525005|\n",
      "|2016-02-18 08:00:00|2016-02-25 08:00:00| 96.09400000000001|\n",
      "|2016-02-25 08:00:00|2016-03-03 08:00:00|         99.276001|\n",
      "|2016-03-03 08:00:00|2016-03-10 08:00:00|101.64000100000001|\n",
      "|2016-03-10 08:00:00|2016-03-17 08:00:00|        104.226001|\n",
      "|2016-03-17 08:00:00|2016-03-24 08:00:00|       106.0699996|\n",
      "|2016-03-24 08:00:00|2016-03-31 08:00:00|       107.8549995|\n",
      "|2016-03-31 08:00:00|2016-04-07 08:00:00|110.08399979999999|\n",
      "|2016-04-07 08:00:00|2016-04-14 08:00:00|       110.4520004|\n",
      "|2016-04-14 08:00:00|2016-04-21 08:00:00|107.46800060000001|\n",
      "|2016-04-21 08:00:00|2016-04-28 08:00:00|       101.5520004|\n",
      "|2016-04-28 08:00:00|2016-05-05 08:00:00|        93.9979994|\n",
      "|2016-05-05 08:00:00|2016-05-12 08:00:00| 92.35599959999999|\n",
      "|2016-05-12 08:00:00|2016-05-19 08:00:00|        93.3299974|\n",
      "+-------------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val thumbling_df = df_2016.\n",
    "                        groupBy(window($\"Date\",\"1 week\")).\n",
    "                        agg(avg($\"Close\").as(\"close_agg\"))\n",
    "                        \n",
    "//print_window(thumbling_df,\"Close\")\n",
    "thumbling_df.sort(\"window.start\").select(\"window.start\",\"window.end\",\"close_agg\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 带有开始时间的Time window\n",
    "上面的输出按照window.start进行了排序，这个字段标记了窗口的开始时间。上面的输出你可能已经看到了第一行的开始时间是2015-12-31，结束时间是2016-01-07。但是你从原始数据可以得到：2016年Apple公司的股票交易信息是从2016-01-04开始的；原因是2016-01-01是元旦，而2016-01-02和2016-01-03正好是周末，期间没有股票交易。\n",
    "\n",
    "我们可以手动指定窗口的开始时间来解决这个问题。\n",
    "```scala\n",
    "def window(timeColumn: Column, windowDuration: String, slideDuration: String, startTime: String){\n",
    "    Bucketize rows into one or more time windows given a timestamp specifying column. Window starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in the order of months are not supported. The following example takes the average stock price for a one minute window every 10 seconds starting 5 seconds after the hour:\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------+\n",
      "|              start|                end|         close_agg|\n",
      "+-------------------+-------------------+------------------+\n",
      "|2015-12-28 08:00:00|2016-01-04 08:00:00|        105.349998|\n",
      "|2016-01-04 08:00:00|2016-01-11 08:00:00|        99.0699982|\n",
      "|2016-01-11 08:00:00|2016-01-18 08:00:00| 98.49999799999999|\n",
      "|2016-01-18 08:00:00|2016-01-25 08:00:00|        98.1220016|\n",
      "|2016-01-25 08:00:00|2016-02-01 08:00:00|        96.2539976|\n",
      "|2016-02-01 08:00:00|2016-02-08 08:00:00| 95.29199960000001|\n",
      "|2016-02-08 08:00:00|2016-02-15 08:00:00|        94.2374975|\n",
      "|2016-02-15 08:00:00|2016-02-22 08:00:00|        96.7880004|\n",
      "|2016-02-22 08:00:00|2016-02-29 08:00:00| 96.23000160000001|\n",
      "|2016-02-29 08:00:00|2016-03-07 08:00:00|101.53200079999999|\n",
      "|2016-03-07 08:00:00|2016-03-14 08:00:00|       101.6199998|\n",
      "|2016-03-14 08:00:00|2016-03-21 08:00:00|105.63600160000001|\n",
      "|2016-03-21 08:00:00|2016-03-28 08:00:00|105.92749950000001|\n",
      "|2016-03-28 08:00:00|2016-04-04 08:00:00|109.46799940000001|\n",
      "|2016-04-04 08:00:00|2016-04-11 08:00:00|109.39799980000001|\n",
      "|2016-04-11 08:00:00|2016-04-18 08:00:00|       110.3820004|\n",
      "|2016-04-18 08:00:00|2016-04-25 08:00:00|106.15400079999999|\n",
      "|2016-04-25 08:00:00|2016-05-02 08:00:00|        96.8759994|\n",
      "|2016-05-02 08:00:00|2016-05-09 08:00:00|        93.6240004|\n",
      "|2016-05-09 08:00:00|2016-05-16 08:00:00| 92.13399799999999|\n",
      "+-------------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// 下面的示例中，4 days参数就是开始时间的偏移量；前两个参数分别代表窗口时间和滑动时间，我们打印出这个窗口的内容：\n",
    "val iteblogWindowWithStartTime = df_2016.\n",
    "                                    groupBy(window($\"Date\",\"1 week\",\"1 week\",\"4 days\")).\n",
    "                                    agg(avg($\"Close\").as(\"close_agg\"))\n",
    "iteblogWindowWithStartTime.sort(\"window.start\").select(\"window.start\",\"window.end\",\"close_agg\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Window api\n",
    "有如下表, We want to answer two questions:\n",
    "1. What are the best-selling and the second best-selling products in every category?  \n",
    "2. What is the difference between the revenue of each product and the revenue of the best-selling product in the same category of that product?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+\n",
      "|   product|  category|revenue|\n",
      "+----------+----------+-------+\n",
      "|      Thin|Cell Phone|   6000|\n",
      "|    Normal|    Tablet|   1500|\n",
      "|      Mini|    Tablet|   5500|\n",
      "|Ultra Thin|Cell Phone|   5000|\n",
      "| Very Thin|Cell Phone|   6000|\n",
      "|       Big|    Tablet|   2500|\n",
      "|  Bendable|Cell Phone|   3000|\n",
      "|  Foldable|Cell Phone|   3000|\n",
      "|       Pro|    Tablet|   4500|\n",
      "|      Pro2|    Tablet|   6500|\n",
      "+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions._\n",
    "val myspark = spark\n",
    "import myspark.implicits._\n",
    "\n",
    "val df = Seq((\"Thin\", \"Cell Phone\",6000), (\"Normal\", \"Tablet\",1500), (\"Mini\",\"Tablet\",5500), \n",
    "             (\"Ultra Thin\",\"Cell Phone\",5000),(\"Very Thin\",\"Cell Phone\",6000),(\"Big\",\"Tablet\",2500), \n",
    "             (\"Bendable\",\"Cell Phone\",3000),(\"Foldable\",\"Cell Phone\",3000), \n",
    "             (\"Pro\",\"Tablet\",4500),(\"Pro2\",\"Tablet\",6500)).\n",
    "         toDF(\"product\", \"category\",\"revenue\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第一个问题: 每个category内的前两名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+----+\n",
      "|   product|  category|revenue|rank|\n",
      "+----------+----------+-------+----+\n",
      "|      Pro2|    Tablet|   6500|   1|\n",
      "|      Mini|    Tablet|   5500|   2|\n",
      "|      Thin|Cell Phone|   6000|   1|\n",
      "| Very Thin|Cell Phone|   6000|   1|\n",
      "|Ultra Thin|Cell Phone|   5000|   2|\n",
      "+----------+----------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// dense_rank():window内的rank按照实际大小递增, 相同的值有相同的rank\n",
    "// rank(): window内的rank按照序列顺序递增, 相同的值会有不同的rank\n",
    "df.createOrReplaceTempView(\"revenue_df\")\n",
    "spark.sql(\"SELECT * FROM (\n",
    "                SELECT product,\n",
    "                       category,\n",
    "                       revenue, \n",
    "                       dense_rank() OVER (PARTITION BY category ORDER BY revenue DESC) as rank \n",
    "                FROM revenue_df \n",
    "        )WHERE rank<=2\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第二个问题: 每个category内与自己category内最大的值得差距"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+----+\n",
      "|   product|  category|revenue| dif|\n",
      "+----------+----------+-------+----+\n",
      "|      Pro2|    Tablet|   6500|   0|\n",
      "|      Mini|    Tablet|   5500|1000|\n",
      "|       Pro|    Tablet|   4500|2000|\n",
      "|       Big|    Tablet|   2500|4000|\n",
      "|    Normal|    Tablet|   1500|5000|\n",
      "|      Thin|Cell Phone|   6000|   0|\n",
      "| Very Thin|Cell Phone|   6000|   0|\n",
      "|Ultra Thin|Cell Phone|   5000|1000|\n",
      "|  Bendable|Cell Phone|   3000|3000|\n",
      "|  Foldable|Cell Phone|   3000|3000|\n",
      "+----------+----------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// DataFrame API\n",
    "val window_space = Window.partitionBy($\"category\").orderBy($\"revenue\".desc)\n",
    "val dif = max($\"revenue\").over(window_space)-$\"revenue\"\n",
    "df.select($\"product\",$\"category\",$\"revenue\",dif.as(\"dif\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+----+\n",
      "|   product|  category|revenue| dif|\n",
      "+----------+----------+-------+----+\n",
      "|      Pro2|    Tablet|   6500|   0|\n",
      "|      Mini|    Tablet|   5500|1000|\n",
      "|       Pro|    Tablet|   4500|2000|\n",
      "|       Big|    Tablet|   2500|4000|\n",
      "|    Normal|    Tablet|   1500|5000|\n",
      "|      Thin|Cell Phone|   6000|   0|\n",
      "| Very Thin|Cell Phone|   6000|   0|\n",
      "|Ultra Thin|Cell Phone|   5000|1000|\n",
      "|  Bendable|Cell Phone|   3000|3000|\n",
      "|  Foldable|Cell Phone|   3000|3000|\n",
      "+----------+----------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// sql\n",
    "spark.sql(\"SELECT * FROM (\n",
    "            SELECT product,\n",
    "                   category,\n",
    "                   revenue, \n",
    "                   max(revenue) OVER (PARTITION BY category ORDER BY revenue DESC)-revenue as dif \n",
    "            FROM revenue_df )\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
