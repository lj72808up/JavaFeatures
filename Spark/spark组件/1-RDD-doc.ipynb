{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. RDD basic\n",
    "reduce把任务发送到不同机器上执行. 这些机器各自持有map结果的一部分, 并在这部分上进行local reduce, 最后将结果发送给driver programme机器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines = data.txt MapPartitionsRDD[12] at textFile at <console>:30\n",
       "lineLengths = MapPartitionsRDD[13] at map at <console>:31\n",
       "totalLength = 7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[13] at map at <console>:31"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"data.txt\")\n",
    "val lineLengths = lines.map(s => s.length)  //懒计算\n",
    "val totalLength = lineLengths.reduce((a, b) => a + b)  // action操作,触发计算\n",
    "lineLengths.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 闭包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter value: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "counter = 0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var counter = 0\n",
    "// Wrong: Don't do this!!\n",
    "lineLengths.foreach(x => {counter += x})\n",
    "println(\"Counter value: \" + counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 如上操作不会正常执行:  \n",
    " 1. spark将rdd操作打散成多个`task`, 每个task由`executor`执行.   \n",
    " 2. 在任务被执行前, spark讲rdd的闭包序列化后发送给每个`executor`, (此处是foreach). 闭包中的变量, 都是复制到`executor`所在机器上的, 每个`executor`操作的也是本地机器上的变量, 而不是driver programme机器上的变量. 因此, 在上述程序输出sounter时, 仍然输出的是driver机器内存里的counter(等于0), 而不是`executor`计算rdd后的输出结果\n",
    " \n",
    "2. Accumulator  \n",
    "像如上这种, 需要在集群中享变量进行和操作的情况, 需要使用`Accumulator`  \n",
    "下面, 我们定义一个long型的Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accum = LongAccumulator(id: 302, name: Some(my long accumlater), value: 15)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accum = sc.longAccumulator(\"my long accumlater\")\n",
    "sc.parallelize(Array(1,2,4,8)).foreach(x=>accum.add(x))\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Printing elements of an RDD\n",
    "1. 闭包中的print只在`executor`上执行, 而不会显现在driver机器上.  \n",
    " 例如: `rdd.foreach(println) or rdd.map(println)`  \n",
    "2. `rdd.collect().foreach(println)`会导致driver机器内存溢出. 因为`collect()`会收集所有executor产生的数据  \n",
    "3. 少量数据打印可使用`rdd.take(100).foreach(println)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Working with Key-Value Pairs\n",
    "1. 有一类专门操作键值对类型RDD的方法, 最普遍的就是`shuffle`, 比如使用key进行grouping或aggregating\n",
    "2. 键值对型RDD中的key, 必须有`equals()`方法, 来匹配`hashCode()`方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22,1)\n",
      "(11,1)\n",
      "(333,1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lines = data.txt MapPartitionsRDD[22] at textFile at <console>:34\n",
       "pairs = MapPartitionsRDD[23] at map at <console>:35\n",
       "counts = ShuffledRDD[24] at reduceByKey at <console>:36\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[24] at reduceByKey at <console>:36"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"data.txt\")\n",
    "val pairs = lines.map(s => (s, 1))\n",
    "val counts = pairs.reduceByKey((a, b) => a + b)  //ShuffledRDD[24] at reduceByKey\n",
    "counts.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. transaction in partition example\n",
    "1. `mapPartitionsWithIndex(func)`:  \n",
    " func(partitionId,iterator_for_each_partition)  \n",
    " 第一个参数为index, 第二个参数为分区内数据的iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partID:0, val: 1\n",
      "partID:0, val: 2\n",
      "partID:0, val: 3\n",
      "partID:0, val: 4\n",
      "partID:1, val: 5\n",
      "partID:1, val: 6\n",
      "partID:1, val: 7\n",
      "partID:1, val: 8\n",
      "partID:1, val: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd1 = ParallelCollectionRDD[32] at parallelize at <console>:30\n",
       "func = > Iterator[String] = <function2>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function2>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2) // 存在2个分区\n",
    "val func = (index: Int, iter: Iterator[(Int)])=>{\n",
    "    iter.toList.map(x => \"partID:\" +  index + \", val: \" + x).iterator\n",
    "}\n",
    "rdd1.mapPartitionsWithIndex(func).collect.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `aggregate(init_value)(func1,func2)`  \n",
    "第一个参数是初始值  \n",
    "第二个参数是2个函数:  \n",
    " 1. 第一个函数:先对个个分区进行的操作  \n",
    " 2. 第二个函数:对个个分区合并后的结果再进行合并,输出一个参数\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//rdd1的分区情况上面已给出\n",
    "rdd1.aggregate(5)(math.max(_, _), _ + _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `aggregateByKey(init_value)(func1,func2)`  \n",
    " 1. 将key值相同的item, 先在分区内进行local aggregate, 再在分区间进行跨分区的aggregate  \n",
    " 2. reduceByKey(+)调用的都是同一个方法，只是aggregateByKey要底层一些，可以先局部再整体操作。\n",
    " 3. `grupByKey`: 按照key分组, 返回(k,Sequence(v)), groupbykey相比reducebykey少了分组后进行聚合操作的函数制定, 因此,groupByKey后, 使用map指定聚合函数对序列操作\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pairRDD = ParallelCollectionRDD[0] at parallelize at <console>:27\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(dog,12), (cat,17), (mouse,6)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pairRDD = sc.parallelize(List((\"cat\",2), (\"cat\", 5), (\"mouse\", 4),(\"cat\", 12), (\"dog\", 12), (\"mouse\", 2))\n",
    "                             , 2)\n",
    "pairRDD.aggregateByKey(0)((x,y)=>math.max(x,y),\n",
    "                          (x,y)=>x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(dog,CompactBuffer(12))\n",
      "(cat,CompactBuffer(2, 5, 12))\n",
      "(mouse,CompactBuffer(4, 2))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd1 = ShuffledRDD[5] at groupByKey at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[5] at groupByKey at <console>:31"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = pairRDD.groupByKey\n",
    "rdd1.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. `countByKey`  \n",
    "根据key计算key的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a,1)\n",
      "(b,2)\n",
      "(c,2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd1 = ParallelCollectionRDD[15] at parallelize at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map((c,2) -> 1, (a,1) -> 1, (b,2) -> 2, (c,1) -> 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(List((\"a\", 1), (\"b\", 2), (\"b\", 2), (\"c\", 2), (\"c\", 1)))\n",
    "rdd1.countByKey.foreach(println)\n",
    "rdd1.countByValue//将(\"a\", 1)当做一个元素，统计其出现的次数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. `cogroup(otherDataset)`   \n",
    " When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable<V>, Iterable<W>)) tuples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(B,(CompactBuffer(2),CompactBuffer()))\n",
      "(D,(CompactBuffer(),CompactBuffer(d)))\n",
      "(A,(CompactBuffer(1),CompactBuffer(a)))\n",
      "(C,(CompactBuffer(3),CompactBuffer(c)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd1 = ParallelCollectionRDD[29] at makeRDD at <console>:31\n",
       "rdd2 = ParallelCollectionRDD[30] at makeRDD at <console>:32\n",
       "rdd3 = MapPartitionsRDD[32] at cogroup at <console>:33\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[32] at cogroup at <console>:33"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var rdd1 = sc.makeRDD(Array((\"A\",\"1\"),(\"B\",\"2\"),(\"C\",\"3\")),2)\n",
    "var rdd2 = sc.makeRDD(Array((\"A\",\"a\"),(\"C\",\"c\"),(\"D\",\"d\")),2)\n",
    "var rdd3 = rdd1.cogroup(rdd2)\n",
    "rdd3.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. `cartesian`  \n",
    "笛卡尔积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a,1)\n",
      "(a,2)\n",
      "(b,1)\n",
      "(c,1)\n",
      "(b,2)\n",
      "(c,2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd1 = ParallelCollectionRDD[39] at makeRDD at <console>:30\n",
       "rdd2 = ParallelCollectionRDD[40] at makeRDD at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[40] at makeRDD at <console>:31"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var rdd1 = sc.makeRDD(Array(\"a\",\"b\",\"c\"),2)\n",
    "var rdd2 = sc.makeRDD(Array(1,2),2)\n",
    "rdd1.cartesian(rdd2).collect.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. `coalesce`与`repartition`   \n",
    "二者均是重新对数据分区.   \n",
    "`repartition`会对所有数据重新shuffle  \n",
    "`coalesce`用于合并分区, 减小分区数量, 用于果过滤大数据集后的搞笑操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. shuffle\n",
    "1. shuffle是对数据进行're-distributing'操作的机制, 让数据通过分区分到不同的组\n",
    "2. shuffle需要跨机器, 跨executors之间进行数据复制, 因此是一种开销极大的操作\n",
    "3. 什么样的操作会引起shuffle:  \n",
    "  1. `repartition` and `coalesce`\n",
    "  2. `ByKey`操作, 除了`countByKey`, 例如`groupByKey` and `reduceByKey`  \n",
    "  3. join操作: `cogroup` and `join`  \n",
    "4. shuffle会引起大量网络IO, 而且当内存中无法存储下组织的数据, 还会引起磁盘IO来缓存数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. RDD Persistence\n",
    "1. RDD的任何一个分区丢失, 都会从源头通过rtansformation操作, 重新计算RDD\n",
    "2. `presist()`可指定持久化级别, 而`cache()`使用默认级别持久化:MEMORY_ONLY(将RDD序列化后存入内存)  \n",
    "3. `MEMORY_ONLY_SER`级别: 可以自己指定持久化序列方式\n",
    "4. 尽量不使用写磁盘的持久化级别, 除非数据计算十分复杂或需要持久化的数据量很大, 从源头重新计算RDD往往更快  \n",
    "5. 可以使用带副本的持久化级别, 如`MEMORY_ONLY_2`, 这会将数据持久化到集群中的2个不同节点.  \n",
    " spark所有持久化都是有容错机制的, 即分区丢失的话, 会重新计算该RDD分区的数据. 而使用带副本的持久化可以在1个分区副本丢失时继续执行任务, 而不必等带这个分区重新计算  \n",
    "6. cache的数据将自动被spark清除出内存(使用LRU算法-least-recently-used). 可以手动使用RDD.unpersist()释放内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Broadcast变量\n",
    "1. Broadcast变量在远程executor机器上共享只读变量(Accumulator是可读可写变量)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "broadcastVar = Broadcast(6)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val broadcastVar = sc.broadcast(Array(1, 2, 3))\n",
    "broadcastVar.value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
