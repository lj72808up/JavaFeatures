{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "// To make some of the examples work we will also need RDD\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.graphx.{Edge, Graph}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Graph的属性\n",
    "#### 1. 图的表示\n",
    "(1) 每个Vertex都是一个kv对, 以64-bit长度的long型值作为key(称作vertexId); 顶点的自定义属性值为value  \n",
    "(2) 每个Edge都有srcVertextId和destVertexId; 还有边上的属性值  \n",
    "(3) **Graph[VD,ED]**是图对象的泛型, VD是定点的属性类型; ED是边的属性类型  \n",
    "(4) 有些case中, vertex的属性可能有多个类型, 此时只能通过继承同一个接口实现 :   \n",
    "    ```scala\n",
    "    class VertexProperty()\n",
    "    case class UserProperty(val name:String) extends VertexProperty\n",
    "    case class ProductProperty(val name:String,val price:Double) extends VertexProperty\n",
    "    var g:Graph[VertexProperty,String] = null\n",
    "    ```\n",
    "#### 2. Graph是静态的\n",
    "(1) 和RDD一样, Graph也是静态的, 任何修改Graph的值或结构的操作, 都会产生一个全新的Graph;   \n",
    "(2) 不过, 源Graph中未产生修改的部分数据可以在新产生的Graph上重用\n",
    "   \n",
    "#### 3. Graph的逻辑表示\n",
    "Graph的逻辑表示为2个RDD集合(源码中VertexRDD和EdgeRDD都继承自RDD类), 如下表示 : \n",
    "<img src=\"img/graphres.png\" width=\"65%\">\n",
    "\n",
    "#### 4. 创建Graph   \n",
    "如下, 用2个RDD创建Graph, 后面会介绍更多的创建Graph的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "users = ParallelCollectionRDD[0] at parallelize at <console>:37\n",
       "relationships = ParallelCollectionRDD[1] at parallelize at <console>:40\n",
       "defaultUser = (Anonymous,Missing)\n",
       "g = org.apache.spark.graphx.impl.GraphImpl@4e301178\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@4e301178"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx.Graph\n",
    "// rdd for vertex\n",
    "val users:RDD[(VertexId,(String,String))] = sc.parallelize(Array((3L, (\"rxin\", \"student\")), \n",
    "                                                                 (7L, (\"jgonzal\", \"postdoc\")),\n",
    "                                                                 (5L, (\"franklin\", \"prof\")), \n",
    "                                                                 (2L, (\"istoica\", \"prof\"))))\n",
    "// rdd for edge\n",
    "val relationships:RDD[Edge[String]] = sc.parallelize(Array(Edge(3l,7l,\"collab\"),\n",
    "                                                           Edge(5L, 3L, \"advisor\"),\n",
    "                                                           Edge(2L, 5L, \"colleague\"), \n",
    "                                                           Edge(5L, 7L, \"pi\")))\n",
    "\n",
    "val defaultUser:(String,String) = (\"Anonymous\",\"Missing\")\n",
    "val g:Graph[(String,String),String] = Graph(users,relationships,defaultUser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cnt1 = 1\n",
       "cnt2 = 1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cnt1 = g.vertices.filter({                            // VertexRDD[VD] extends RDD[(VertexId, VD)]\n",
    "  case (id,(name,position)) => position == \"postdoc\"      // EdgeRDD[ED] extends RDD[Edge[ED]](sc, deps)\n",
    "}).count\n",
    "\n",
    "val cnt2 = g.edges.filter(e=>e.srcId>e.dstId).count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Graph的三元组视图\n",
    "(1) 除了从Rdd[Long,VD]和RDD[Long,Long,ED]的角度看图的构成外, Graph还有一个三元组视图:   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(1) (srcId,srcAttr),  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(2) (dstId,dstAttr),  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(3) attr    \n",
    "(2) 三元组可以看做join得来的视图\n",
    "```sql\n",
    "SELECT src.id, dst.id, src.attr, e.attr, dst.attr\n",
    "FROM edges AS e LEFT JOIN vertices AS src, vertices AS dst\n",
    "ON e.srcId = src.Id AND e.dstId = dst.Id\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((3,(rxin,student)),(7,(jgonzal,postdoc)),collab)\n",
      "((5,(franklin,prof)),(3,(rxin,student)),advisor)\n",
      "((2,(istoica,prof)),(5,(franklin,prof)),colleague)\n",
      "((5,(franklin,prof)),(7,(jgonzal,postdoc)),pi)\n"
     ]
    }
   ],
   "source": [
    "/**\n",
    " * class EdgeTriplet[VD, ED]{\n",
    " *    override def toString: String = ((srcId, srcAttr), (dstId, dstAttr), attr).toString()\n",
    " * } \n",
    " */\n",
    "g.triplets.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 操作符\n",
    "#### 1. map操作符\n",
    "每种map操作都会产生一个新的Graph, 不过会重用map之前的Graph的部分数据\n",
    "```scala\n",
    "class Graph{\n",
    "    def mapVertices[VD2: ClassTag](map: (VertexId, VD) => VD2)\n",
    "    def mapEdges[ED2: ClassTag](map: Edge[ED] => ED2): Graph[VD, ED2]\n",
    "    def mapTriplets[ED2: ClassTag](map: EdgeTriplet[VD, ED] => ED2): Graph[VD, ED2]\n",
    "}\n",
    "```\n",
    "\n",
    "#### 2. 结构操作符\n",
    "(1) reverse: Graph[VD, ED]   \n",
    "将边的src和dst互换; 由于改变边的属性和边的条数, 可以很快速地实现\n",
    "```scala\n",
    "def reverse: Graph[VD, ED]\n",
    "```\n",
    "  \n",
    "(2) subgraph: graph中只保留\n",
    "```scala\n",
    "def subgraph(\n",
    "      epred: EdgeTriplet[VD, ED] => Boolean = (x => true),\n",
    "      vpred: (VertexId, VD) => Boolean = ((v, d) => true))\n",
    "    : Graph[VD, ED]\n",
    "```\n",
    "(3) mask: 构建一个子图, 只保留另一个图中也出现的顶点和边\n",
    "```scala\n",
    "def mask[VD2: ClassTag, ED2: ClassTag](other: Graph[VD2, ED2]): Graph[VD, ED]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((3,(rxin,student)),(7,(jgonzal,postdoc)),collab)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "users = ParallelCollectionRDD[77] at parallelize at <console>:46\n",
       "relationships = ParallelCollectionRDD[78] at parallelize at <console>:51\n",
       "defaultUser = (Anonymous,Missing)\n",
       "g = org.apache.spark.graphx.impl.GraphImpl@5d140851\n",
       "validGraph = org.apache.spark.graphx.impl.GraphImpl@1806e871\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@1806e871"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx.Graph\n",
    "// rdd for vertex\n",
    "val users:RDD[(VertexId,(String,String))] = sc.parallelize(Array((3L, (\"rxin\", \"student\")), \n",
    "                                                                 (7L, (\"jgonzal\", \"postdoc\")),\n",
    "                                                                 (5L, (\"franklin\", \"prof\")), \n",
    "                                                                 (2L, (\"istoica\", \"prof\"))))\n",
    "// rdd for edge\n",
    "val relationships:RDD[Edge[String]] = sc.parallelize(Array(Edge(3l,7l,\"collab\"),\n",
    "                                                           Edge(5L, 3L, \"advisor\"),\n",
    "                                                           Edge(2L, 5L, \"colleague\"), \n",
    "                                                           Edge(5L, 7L, \"pi\")))\n",
    "\n",
    "val defaultUser:(String,String) = (\"Anonymous\",\"Missing\")\n",
    "val g:Graph[(String,String),String] = Graph(users,relationships,defaultUser)\n",
    "val validGraph = g.subgraph(vpred = (vertexId,attr)=> attr._2 != \"prof\")\n",
    "validGraph.triplets.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. join操作符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Pregel\n",
    "#### 前言\n",
    "1. Graphx有很多的内部优化, 具体参考[graphx paper](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-gonzalez.pdf)\n",
    "\n",
    "#### 一. Pregel api\n",
    "1. 三种user defined function\n",
    "    1. **Vertex Program**:   \n",
    "       Vertex Program在每个ertex上运行, 其输入为\n",
    "        1. message list\n",
    "        2. vertex attr state\n",
    "        3. vertexId  \n",
    "       输出为vertex的新状态(新的attr)\n",
    "    2. **Send Message Program**:  \n",
    "       运行在需要的边上;其输入为三元组视图**EdgeTriplet**, 输出为1个message\n",
    "    3. **Merge Message Program**:    \n",
    "       把同一个顶点上的2个message合并为一个message, 输出组合后的message  \n",
    "       消息的形式为kv对: (vartexid作为key, vertex message作为value)\n",
    "2. 三个参数\n",
    "    1. Initial message:该message会发送给每个vertex, 用于第一次迭代  \n",
    "    2. Max Iteration: 最大迭代次数    \n",
    "    3. Edge Direction: 用于过滤那些需要执行send message程序的边上; 只有当变得方向是OUT是才会执行发送程序\n",
    "    \n",
    "#### 二. Pregel的表现优化\n",
    "1. VertexRDD手动分区   \n",
    "    graphx只会对EdgeRDD分区, 因此需要手动对VertexRDD分区; 经验上看, VertexRDD和EdgeRDD个数相同时会有更好的表现\n",
    "2. 设置checkpoint    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;因为graphx是迭代算法, 每次迭代都会导致构成graph的VertexRDD和EdgeRDD链会越来越长; 所以需要使用缓存来确保每次迭代避免重复计算RDD链;单着并不能改变一个事实: 子RDD到父RDD的对象引用列表还是会不断增长. 为了切断RDD的linage, 应该在每几次迭代后进行checkpoint.  \n",
    "3. 如下,有checkpoint的迭代式图更新算法(模拟) :    \n",
    "pregel中每次迭代会persist到内存  ; 每隔一段间隔checkpint;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Iteration 1\n",
      "4\n",
      "Iteration 2\n",
      "4\n",
      "Iteration 3\n",
      "4\n",
      "Iteration 4\n",
      "4\n",
      "Iteration 5\n",
      "4\n",
      "Iteration 6\n",
      "4\n",
      "Iteration 7\n",
      "4\n",
      "Iteration 8\n",
      "4\n",
      "Iteration 9\n",
      "4\n",
      "Iteration 10\n",
      "4\n",
      "Iteration 11\n",
      "4\n",
      "Iteration 12\n",
      "4\n",
      "Iteration 13\n",
      "4\n",
      "Iteration 14\n",
      "4\n",
      "Iteration 15\n",
      "4\n",
      "Iteration 16\n",
      "4\n",
      "Iteration 17\n",
      "4\n",
      "Iteration 18\n",
      "4\n",
      "Iteration 19\n",
      "4\n",
      "Iteration 20\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fun: ()Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1,0),(3,0),1)\n",
      "((2,0),(4,0),1)\n",
      "((3,0),(4,0),1)\n"
     ]
    }
   ],
   "source": [
    "sc.setLogLevel(\"WARN\")\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "\n",
    "def fun() = {\n",
    "    sc.setCheckpointDir(\"/tmp/test\")\n",
    "    var updateCount = 0\n",
    "    val interval = 10\n",
    "\n",
    "    def update(data:Graph[Int,Int]):Unit = {\n",
    "      data.persist()  // 每轮迭代都persist\n",
    "      updateCount += 1\n",
    "      if(updateCount%interval == 0)   // 每隔interval进行checkpoint\n",
    "        data.checkpoint()\n",
    "    }\n",
    "\n",
    "    var g = Graph.fromEdges(sc.parallelize(Array(Edge(1l,3l,1),\n",
    "      Edge(2l,4l,1),\n",
    "      Edge(3l,4l,1))),1)\n",
    "\n",
    "    g.persist()\n",
    "    println(g.vertices.count())\n",
    "\n",
    "    for(i <- 1 to 20){\n",
    "      println(s\"Iteration $i\")\n",
    "      val newGraph = g.mapVertices((vid,vattr) => (vattr*i)/17)\n",
    "      g = g.outerJoinVertices(newGraph.vertices)({(vid,vAttr,newAttr) => newAttr.getOrElse(-99)})\n",
    "      update(g)\n",
    "      println(g.vertices.count)\n",
    "    }\n",
    "\n",
    "    g.triplets.collect.foreach(println)\n",
    "}\n",
    "fun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
