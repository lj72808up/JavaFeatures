{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "// To make some of the examples work we will also need RDD\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.graphx.{Edge, Graph}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Graph的属性\n",
    "#### 1. 图的表示\n",
    "(1) 每个Vertex都是一个kv对, 以64-bit长度的long型值作为key(称作vertexId); 顶点的自定义属性值为value  \n",
    "(2) 每个Edge都有srcVertextId和destVertexId; 还有边上的属性值  \n",
    "(3) **Graph[VD,ED]**是图对象的泛型, VD是定点的属性类型; ED是边的属性类型  \n",
    "(4) 有些case中, vertex的属性可能有多个类型, 此时只能通过继承同一个接口实现 :   \n",
    "    ```scala\n",
    "    class VertexProperty()\n",
    "    case class UserProperty(val name:String) extends VertexProperty\n",
    "    case class ProductProperty(val name:String,val price:Double) extends VertexProperty\n",
    "    var g:Graph[VertexProperty,String] = null\n",
    "    ```\n",
    "#### 2. Graph是静态的\n",
    "(1) 和RDD一样, Graph也是静态的, 任何修改Graph的值或结构的操作, 都会产生一个全新的Graph;   \n",
    "(2) 不过, 源Graph中未产生修改的部分数据可以在新产生的Graph上重用\n",
    "   \n",
    "#### 3. Graph的逻辑表示\n",
    "Graph的逻辑表示为2个RDD集合(源码中VertexRDD和EdgeRDD都继承自RDD类), 如下表示 : \n",
    "<img src=\"img/graphres.png\" width=\"65%\">\n",
    "\n",
    "#### 4. 创建Graph   \n",
    "如下, 用2个RDD创建Graph, 后面会介绍更多的创建Graph的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "users = ParallelCollectionRDD[0] at parallelize at <console>:37\n",
       "relationships = ParallelCollectionRDD[1] at parallelize at <console>:42\n",
       "defaultUser = (Anonymous,Missing)\n",
       "g = org.apache.spark.graphx.impl.GraphImpl@4cfbf02\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@4cfbf02"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx.Graph\n",
    "// rdd for vertex\n",
    "val users:RDD[(VertexId,(String,String))] = sc.parallelize(Array((3L, (\"rxin\", \"student\")), \n",
    "                                                                 (7L, (\"jgonzal\", \"postdoc\")),\n",
    "                                                                 (5L, (\"franklin\", \"prof\")), \n",
    "                                                                 (2L, (\"istoica\", \"prof\"))))\n",
    "// rdd for edge\n",
    "val relationships:RDD[Edge[String]] = sc.parallelize(Array(Edge(3l,7l,\"collab\"),\n",
    "                                                           Edge(5L, 3L, \"advisor\"),\n",
    "                                                           Edge(2L, 5L, \"colleague\"), \n",
    "                                                           Edge(5L, 7L, \"pi\")))\n",
    "\n",
    "val defaultUser:(String,String) = (\"Anonymous\",\"Missing\")\n",
    "val g:Graph[(String,String),String] = Graph(users,relationships,defaultUser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cnt1 = 1\n",
       "cnt2 = 1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cnt1 = g.vertices.filter({                            // VertexRDD[VD] extends RDD[(VertexId, VD)]\n",
    "  case (id,(name,position)) => position == \"postdoc\"      // EdgeRDD[ED] extends RDD[Edge[ED]](sc, deps)\n",
    "}).count\n",
    "\n",
    "val cnt2 = g.edges.filter(e=>e.srcId>e.dstId).count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Graph的三元组视图\n",
    "(1) 除了从Rdd[Long,VD]和RDD[Long,Long,ED]的角度看图的构成外, Graph还有一个三元组视图:   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(1) (srcId,srcAttr),  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(2) (dstId,dstAttr),  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(3) attr    \n",
    "(2) 三元组可以看做join得来的视图\n",
    "```sql\n",
    "SELECT src.id, dst.id, src.attr, e.attr, dst.attr\n",
    "FROM edges AS e LEFT JOIN vertices AS src, vertices AS dst\n",
    "ON e.srcId = src.Id AND e.dstId = dst.Id\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((3,(rxin,student)),(7,(jgonzal,postdoc)),collab)\n",
      "((5,(franklin,prof)),(3,(rxin,student)),advisor)\n",
      "((2,(istoica,prof)),(5,(franklin,prof)),colleague)\n",
      "((5,(franklin,prof)),(7,(jgonzal,postdoc)),pi)\n"
     ]
    }
   ],
   "source": [
    "/**\n",
    " * class EdgeTriplet[VD, ED]{\n",
    " *    override def toString: String = ((srcId, srcAttr), (dstId, dstAttr), attr).toString()\n",
    " * } \n",
    " */\n",
    "g.triplets.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 操作符\n",
    "#### 1. map操作符\n",
    "每种map操作都会产生一个新的Graph, 不过会重用map之前的Graph的部分数据\n",
    "```scala\n",
    "class Graph{\n",
    "    def mapVertices[VD2: ClassTag](map: (VertexId, VD) => VD2)\n",
    "    def mapEdges[ED2: ClassTag](map: Edge[ED] => ED2): Graph[VD, ED2]\n",
    "    def mapTriplets[ED2: ClassTag](map: EdgeTriplet[VD, ED] => ED2): Graph[VD, ED2]\n",
    "}\n",
    "```\n",
    "\n",
    "#### 2. 结构操作符\n",
    "(1) reverse: Graph[VD, ED]   \n",
    "将边的src和dst互换; 由于改变边的属性和边的条数, 可以很快速地实现\n",
    "```scala\n",
    "def reverse: Graph[VD, ED]\n",
    "```\n",
    "  \n",
    "(2) subgraph: graph中只保留\n",
    "```scala\n",
    "def subgraph(\n",
    "      epred: EdgeTriplet[VD, ED] => Boolean = (x => true),\n",
    "      vpred: (VertexId, VD) => Boolean = ((v, d) => true))\n",
    "    : Graph[VD, ED]\n",
    "```\n",
    "(3) mask: 构建一个子图, 只保留另一个图中也出现的顶点和边\n",
    "```scala\n",
    "def mask[VD2: ClassTag, ED2: ClassTag](other: Graph[VD2, ED2]): Graph[VD, ED]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((3,(rxin,student)),(7,(jgonzal,postdoc)),collab)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "users = ParallelCollectionRDD[23] at parallelize at <console>:43\n",
       "relationships = ParallelCollectionRDD[24] at parallelize at <console>:48\n",
       "defaultUser = (Anonymous,Missing)\n",
       "g = org.apache.spark.graphx.impl.GraphImpl@42769676\n",
       "validGraph = org.apache.spark.graphx.impl.GraphImpl@50fb7127\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@50fb7127"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx.Graph\n",
    "// rdd for vertex\n",
    "val users:RDD[(VertexId,(String,String))] = sc.parallelize(Array((3L, (\"rxin\", \"student\")), \n",
    "                                                                 (7L, (\"jgonzal\", \"postdoc\")),\n",
    "                                                                 (5L, (\"franklin\", \"prof\")), \n",
    "                                                                 (2L, (\"istoica\", \"prof\"))))\n",
    "// rdd for edge\n",
    "val relationships:RDD[Edge[String]] = sc.parallelize(Array(Edge(3l,7l,\"collab\"),\n",
    "                                                           Edge(5L, 3L, \"advisor\"),\n",
    "                                                           Edge(2L, 5L, \"colleague\"), \n",
    "                                                           Edge(5L, 7L, \"pi\")))\n",
    "\n",
    "val defaultUser:(String,String) = (\"Anonymous\",\"Missing\")\n",
    "val g:Graph[(String,String),String] = Graph(users,relationships,defaultUser)\n",
    "val validGraph = g.subgraph(vpred = (vertexId,attr)=> attr._2 != \"prof\")\n",
    "validGraph.triplets.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. join操作符\n",
    "1. joinVertices: \n",
    "    1. 以vertexId为key进行关联, \n",
    "    2. 需要定义一个map函数, 其输出的类型和vertex attr的类型要保持一致\n",
    "        **map: (VertexId, VD, U) => VD** : VD是原vertexRDD的attr属性, U是join的RDD的Option值\n",
    "    ```scala\n",
    "    class Graph[VD, ED] {\n",
    "      def joinVertices[U](table: RDD[(VertexId, U)])(map: (VertexId, VD, U) => VD) : Graph[VD, ED]\n",
    "    }\n",
    "    ```\n",
    "2. outerJoinVertices\n",
    "   outerJoinVertices与joinVertices大致相同, 只有2个不一样的地方: \n",
    "    1. **map: (VertexId, VD, Option[U]) => VD2** : 其中输入的第三个参数为Option类型\n",
    "    2. map函数输出的类型可以任意, 不必和原vertexAttr一致\n",
    "        ```scala\n",
    "    class Graph[VD, ED] {\n",
    "      def outerJoinVertices[U, VD2](table: RDD[(VertexId, U)])(map: (VertexId, VD, Option[U]) => VD2) : Graph[VD2, ED]\n",
    "    }\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((3,(strings,(rxin,student)_out:1)),(7,(jgonzal,postdoc)),collab)\n",
      "((5,(strings,(franklin,prof)_out:2)),(3,(strings,(rxin,student)_out:1)),advisor)\n",
      "((2,(strings,(istoica,prof)_out:1)),(5,(strings,(franklin,prof)_out:2)),colleague)\n",
      "((5,(strings,(franklin,prof)_out:2)),(7,(jgonzal,postdoc)),pi)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "outDegrees = VertexRDDImpl[49] at RDD at VertexRDD.scala:57\n",
       "newG = org.apache.spark.graphx.impl.GraphImpl@291d0b55\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@291d0b55"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val outDegrees = g.outDegrees\n",
    "val newG = g.joinVertices(outDegrees)({\n",
    "    (vid,vattr,newAttr) => (\"strings\",s\"${vattr}_out:${newAttr}\") // 原vertexAttr为(String,String), 则map输出也应该是(String,String)\n",
    "})\n",
    "newG.triplets.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((3,999),(7,999),collab)\n",
      "((5,999),(3,999),advisor)\n",
      "((2,999),(5,999),colleague)\n",
      "((5,999),(7,999),pi)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "outDegrees = VertexRDDImpl[49] at RDD at VertexRDD.scala:57\n",
       "newG = org.apache.spark.graphx.impl.GraphImpl@365cacbf\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@365cacbf"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val outDegrees = g.outDegrees\n",
    "val newG = g.outerJoinVertices(outDegrees)({\n",
    "    (vid,vattr,newAttrOpt) => 999   // map函数可任意输出\n",
    "})\n",
    "newG.triplets.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Nerghborhood Aggregation\n",
    "(1) 很多图迭代模型, 都要聚合来自邻居节点的消息, 因此, graphx提供对应的方法aggregateMessages  \n",
    "(2) aggregateMessages需要定义sendMsg()和mergeMsg(); 返回值为VertexRDD    \n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sendMsg输入中提供了EdgeContext获取边的选相关信息(如下析构函数), 和sendToSrc,sendToDst方法  \n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mergeMsg相当于聚合2个msg为1个msg的reduce函数\n",
    "```scala\n",
    "def aggregateMessages[Msg: ClassTag](\n",
    "  sendMsg: EdgeContext[VD, ED, Msg] => Unit,\n",
    "  mergeMsg: (Msg, Msg) => Msg,\n",
    "  tripletFields: TripletFields = TripletFields.All)\n",
    ": VertexRDD[Msg]\n",
    "\n",
    "class EdgeContext {\n",
    "  def sendToSrc(msg: A): Unit\n",
    "  def sendToDst(msg: A): Unit\n",
    "}\n",
    "object EdgeContext {\n",
    "    /** EdgeContext中包含的属性 */\n",
    "    def unapply[VD, ED, A](edge: EdgeContext[VD, ED, A]): Some[(VertexId, VertexId, VD, VD, ED)] =\n",
    "        Some((edge.srcId, edge.dstId, edge.srcAttr, edge.dstAttr, edge.attr))\n",
    "}\n",
    "```\n",
    "(3) 如下, 每个顶点的vertexAttr都是指代年龄, 使用aggregateMessages实现计算顶点及其周围顶点的年龄平均值. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,6.8)\n",
      "(12,15.0)\n",
      "(13,16.5)\n",
      "(1,9.4)\n",
      "(14,17.0)\n",
      "(2,11.0)\n",
      "(15,18.0)\n",
      "(3,9.857142857142858)\n",
      "(4,10.666666666666666)\n",
      "(16,19.0)\n",
      "(5,13.166666666666666)\n",
      "(6,12.11111111111111)\n",
      "(7,13.0)\n",
      "(8,9.0)\n",
      "(9,14.857142857142858)\n",
      "(10,15.285714285714286)\n",
      "(11,15.166666666666666)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "g = org.apache.spark.graphx.impl.GraphImpl@7394877d\n",
       "vertexs = VertexRDDImpl[211] at RDD at VertexRDD.scala:57\n",
       "avgAgeOfOlderFollowers = VertexRDDImpl[213] at RDD at VertexRDD.scala:57\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VertexRDDImpl[213] at RDD at VertexRDD.scala:57"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx.util.GraphGenerators\n",
    "// 创建1个20个节点, vertexAttr为double型的定点RDD\n",
    "val g:Graph[Double,Int] = GraphGenerators.logNormalGraph(sc,numVertices = 20).mapVertices((age,_)=>age.toDouble)\n",
    "//g.triplets.collect.foreach(println)\n",
    "val vertexs: VertexRDD[(Int, Double)] =  g.aggregateMessages[(Int,Double)](  // vertexAtt的类型为(Int,Double)\n",
    "  context=>{  // sendMsg\n",
    "    if(context.srcAttr > context.dstAttr)\n",
    "      context.sendToDst((1,context.srcAttr))\n",
    "  },\n",
    "  (msg1,msg2)=>{  // mergeMsg\n",
    "    (msg1._1+msg2._1,msg1._2+msg2._2)\n",
    "  }\n",
    ")\n",
    "// Divide total age by number of older followers to get average age of older followers\n",
    "val avgAgeOfOlderFollowers: VertexRDD[Double] =\n",
    "  vertexs.mapValues( (vId, value) =>  // mapValues结果中带着vertexId\n",
    "    value match { case (count, totalAge) => totalAge / count } )\n",
    "// Display the results\n",
    "avgAgeOfOlderFollowers.collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Pregel\n",
    "#### 前言\n",
    "1. Graphx有很多的内部优化, 具体参考[graphx paper](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-gonzalez.pdf)\n",
    "2. 关于图的迭代算法, 最好使用pregel api实现, 因为提供了正确的persist,unpersist,checkpoint的流程  \n",
    "    1. persist和unpersist每轮迭代都会进行, 在内存中进行cache; 每次都是先unpersist上次的计算结果, 在persist这次的计算结果  \n",
    "    2. checkpoint是通过参数指定的多少轮以后自动进行 , set **spark.graphx.pregel.checkpointInterval** = 10\n",
    "\n",
    "#### 一. Pregel api\n",
    "1. 三种user defined function\n",
    "    1. **Vertex Program**:   \n",
    "       Vertex Program在每个ertex上运行, 其输入为\n",
    "        1. message list\n",
    "        2. vertex attr state\n",
    "        3. vertexId  \n",
    "       输出为vertex的新状态(新的attr)\n",
    "    2. **Send Message Program**:  \n",
    "       运行在三元组视图的记录上;其输入为三元组视图**EdgeTriplet**, 输出为1个message\n",
    "    3. **Merge Message Program**:    \n",
    "       把同一个顶点上的2个message合并为一个message, 输出组合后的message  \n",
    "       消息的形式为kv对: (vartexid作为key, vertex message作为value)\n",
    "2. 三个参数\n",
    "    1. Initial message:该message会发送给每个vertex, 用于第一次迭代  \n",
    "    2. Max Iteration: 最大迭代次数    \n",
    "    3. Edge Direction: 用于过滤那些需要执行send message程序的边上; 只有当变得方向是OUT是才会执行发送程序\n",
    "    \n",
    "```scala\n",
    "def pregel[A: ClassTag](\n",
    "      initialMsg: A,\n",
    "      maxIterations: Int = Int.MaxValue,   // 默认值\n",
    "      activeDirection: EdgeDirection = EdgeDirection.Either)  // 默认值\n",
    "      (\n",
    "      vprog: (VertexId, VD, A) => VD,\n",
    "      sendMsg: EdgeTriplet[VD, ED] => Iterator[(VertexId, A)],\n",
    "      mergeMsg: (A, A) => A)\n",
    ": Graph[VD, ED]\n",
    "```\n",
    "    \n",
    "#### 二. Pregel的表现优化\n",
    "1. VertexRDD手动分区   \n",
    "    graphx只会对EdgeRDD分区, 因此需要手动对VertexRDD分区; 经验上看, VertexRDD和EdgeRDD个数相同时会有更好的表现\n",
    "2. 设置checkpoint    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;因为graphx是迭代算法, 每次迭代都会导致构成graph的VertexRDD和EdgeRDD链会越来越长; 所以需要使用缓存来确保每次迭代避免重复计算RDD链;单着并不能改变一个事实: 子RDD到父RDD的对象引用列表还是会不断增长. 为了切断RDD的linage, 应该在每几次迭代后进行checkpoint.  \n",
    "3. 如下,有checkpoint的迭代式图更新算法(模拟) :    \n",
    "pregel中每次迭代会persist到内存  ; 每隔一段间隔checkpint;\n",
    "\n",
    "#### 三. 如下使用pregel api进行单源最短路径查找"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0,7),(1,4),1.0)\n",
      "((0,7),(1,4),1.0)\n",
      "((0,7),(6,6),1.0)\n",
      "((0,7),(7,6),1.0)\n",
      "((0,7),(7,6),1.0)\n",
      "((0,7),(8,2),1.0)\n",
      "((0,7),(8,2),1.0)\n",
      "((1,4),(1,4),1.0)\n",
      "((1,4),(2,7),1.0)\n",
      "((1,4),(6,6),1.0)\n",
      "((1,4),(9,8),1.0)\n",
      "((2,7),(0,7),1.0)\n",
      "((2,7),(0,7),1.0)\n",
      "((2,7),(2,7),1.0)\n",
      "((2,7),(6,6),1.0)\n",
      "((2,7),(7,6),1.0)\n",
      "((2,7),(7,6),1.0)\n",
      "((2,7),(8,2),1.0)\n",
      "((3,1),(3,1),1.0)\n",
      "((4,2),(0,7),1.0)\n",
      "((4,2),(5,3),1.0)\n",
      "((5,3),(2,7),1.0)\n",
      "((5,3),(2,7),1.0)\n",
      "((5,3),(9,8),1.0)\n",
      "((6,6),(0,7),1.0)\n",
      "((6,6),(5,3),1.0)\n",
      "((6,6),(6,6),1.0)\n",
      "((6,6),(8,2),1.0)\n",
      "((6,6),(8,2),1.0)\n",
      "((6,6),(9,8),1.0)\n",
      "((7,6),(0,7),1.0)\n",
      "((7,6),(0,7),1.0)\n",
      "((7,6),(1,4),1.0)\n",
      "((7,6),(3,1),1.0)\n",
      "((7,6),(5,3),1.0)\n",
      "((7,6),(7,6),1.0)\n",
      "((8,2),(3,1),1.0)\n",
      "((8,2),(6,6),1.0)\n",
      "((9,8),(0,7),1.0)\n",
      "((9,8),(2,7),1.0)\n",
      "((9,8),(4,2),1.0)\n",
      "((9,8),(7,6),1.0)\n",
      "((9,8),(7,6),1.0)\n",
      "((9,8),(8,2),1.0)\n",
      "((9,8),(8,2),1.0)\n",
      "((9,8),(9,8),1.0)\n",
      "(0,2.0)\n",
      "(1,3.0)\n",
      "(2,3.0)\n",
      "(3,1.0)\n",
      "(4,3.0)\n",
      "(5,2.0)\n",
      "(6,1.0)\n",
      "(7,3.0)\n",
      "(8,0.0)\n",
      "(9,2.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "graph = org.apache.spark.graphx.impl.GraphImpl@617118e6\n",
       "sourceId = 8\n",
       "initialGraph = org.apache.spark.graphx.impl.GraphImpl@ba5b223\n",
       "sssp = org.apache.spark.graphx.impl.GraphImpl@29f3e54\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@29f3e54"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx.{Graph, VertexId}\n",
    "import org.apache.spark.graphx.util.GraphGenerators\n",
    "\n",
    "// A graph with edge attributes containing distances,边的默认值是1.0\n",
    "val graph: Graph[Long, Double] =\n",
    "  GraphGenerators.logNormalGraph(sc, numVertices = 10).mapEdges(e => e.attr.toDouble)\n",
    "graph.triplets.collect.foreach(println)\n",
    "val sourceId: VertexId = 8 // 起始点\n",
    "\n",
    "// 初始化vertexAttr, 除8号vertex外, 其余所有定点的属性值都是无限\n",
    "val initialGraph = graph.mapVertices((id, _) => if (id == sourceId) 0.0 else Double.PositiveInfinity)\n",
    "\n",
    "val sssp = initialGraph.pregel(Double.PositiveInfinity)(\n",
    "  (id, dist, newDist) => math.min(dist, newDist), // Vertex Program\n",
    "  triplet => {  // Send Message, 在边上传播, 框架内部会指找到2次结果不变的边\n",
    "    if (triplet.srcAttr + triplet.attr < triplet.dstAttr) {\n",
    "      Iterator((triplet.dstId, triplet.srcAttr + triplet.attr))\n",
    "    } else {\n",
    "      Iterator.empty\n",
    "    }\n",
    "  },\n",
    "  (a, b) => math.min(a, b) // Merge Message\n",
    ")\n",
    "println(sssp.vertices.collect.mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. pregel的实现思路及其他图算法\n",
    "1. 基本思路\n",
    "    1. 首先, pregel设置一个舒适message发往图中的每个顶点, 每个顶点在收到消息后, 和自身的vertexAttr进行聚合, 作为自己的新attr  \n",
    "    2. 然后, 开始迭代部分:  \n",
    "        每个顶点会想自己的邻居发送messgae, 因此每个节点会受到其邻居发来的message 列表; 节点将这个message列表聚合成一条message更新自己的VertexAttr. 这个步骤称作一个superstep\n",
    "    3. 如果2次super step中, 顶点的vertexAttr没有发生变化, 则认为顶点已经稳定, 下次该顶点既不会发送message, 也不会再受到message\n",
    "    4. 当所有顶点的vertexAttr都不在发生变化时, 算法结束\n",
    "    \n",
    "2. PageRank 在pregel的实现  \n",
    "    1. 每个顶点的vertexAttr初始化为1/$N_{graph顶点个数}$\n",
    "    2. 将自己的vertexAttr传播给邻居\n",
    "    3. 邻居更新自己的attr为$0.15*\\frac{1}{N_{graph顶点个数}}+0.85\\Sigma(邻居的attr)$\n",
    "\n",
    "3. 单源最短路径问题\n",
    "    1. 除源点外, 其余顶点的attr设置为无穷, 边attr设置为1\n",
    "    2. 更新顶点的vertexAttr为(邻居vertexAttr+边的edgeAttr), 如果后者小的话\n",
    "    3. 持续迭代直到收敛\n",
    "    \n",
    "4. 半群问题  \n",
    "    1. 半群问题常用在社交网络中, 表示1个group中的人交往频繁, 而和其他人交往的不频繁;  \n",
    "        半群问题的graph中边有各种不同的权值, 表示顶点之间交互程度的强弱;   \n",
    "        和一般的cluster算法不同, 半群问题下的顶点可能同属多个cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Graph构建起\n",
    "#### 1. GraphLoader 读取文件\n",
    "(1) 文件格式为(source vertex ID, destination vertex ID)对   \n",
    "(2) 自动跳过首行\"#\"注释\n",
    "```text\n",
    "# This is a comment\n",
    "2 1\n",
    "4 1\n",
    "1 2\n",
    "```\n",
    "(3) 读取文件的api   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; a. 如下, 所有顶点和边的attr都是默认的1;  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b. canonicalOrientation参数允许边的方向为正向(srcId<dstId)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; c. 此种方法创建的图不会对edge重分区, 如edgelist在那个block, 就会产生在哪台机器上\n",
    "```scala\n",
    "object GraphLoader {\n",
    "  def edgeListFile(\n",
    "      sc: SparkContext,\n",
    "      path: String,\n",
    "      canonicalOrientation: Boolean = false,\n",
    "      minEdgePartitions: Int = 1)\n",
    "    : Graph[Int, Int]\n",
    "}\n",
    "```\n",
    "#### 2. 从RDD创建图\n",
    "```scala\n",
    "object Graph {\n",
    "  def apply[VD, ED](\n",
    "      vertices: RDD[(VertexId, VD)],\n",
    "      edges: RDD[Edge[ED]],\n",
    "      defaultVertexAttr: VD = null)\n",
    "    : Graph[VD, ED]\n",
    "\n",
    "  def fromEdges[VD, ED](\n",
    "      edges: RDD[Edge[ED]],\n",
    "      defaultValue: VD): Graph[VD, ED]\n",
    "\n",
    "  def fromEdgeTuples[VD](\n",
    "      rawEdges: RDD[(VertexId, VertexId)],\n",
    "      defaultValue: VD,\n",
    "      uniqueEdges: Option[PartitionStrategy] = None): Graph[VD, Int]\n",
    "\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[附录]:**  图迭代进行缓存的一般写法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Iteration 1\n",
      "4\n",
      "Iteration 2\n",
      "4\n",
      "Iteration 3\n",
      "4\n",
      "Iteration 4\n",
      "4\n",
      "Iteration 5\n",
      "4\n",
      "Iteration 6\n",
      "4\n",
      "Iteration 7\n",
      "4\n",
      "Iteration 8\n",
      "4\n",
      "Iteration 9\n",
      "4\n",
      "Iteration 10\n",
      "4\n",
      "Iteration 11\n",
      "4\n",
      "Iteration 12\n",
      "4\n",
      "Iteration 13\n",
      "4\n",
      "Iteration 14\n",
      "4\n",
      "Iteration 15\n",
      "4\n",
      "Iteration 16\n",
      "4\n",
      "Iteration 17\n",
      "4\n",
      "Iteration 18\n",
      "4\n",
      "Iteration 19\n",
      "4\n",
      "Iteration 20\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fun: ()Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1,0),(3,0),1)\n",
      "((2,0),(4,0),1)\n",
      "((3,0),(4,0),1)\n"
     ]
    }
   ],
   "source": [
    "sc.setLogLevel(\"WARN\")\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "\n",
    "def fun() = {\n",
    "    sc.setCheckpointDir(\"/tmp/test\")\n",
    "    var updateCount = 0\n",
    "    val interval = 10\n",
    "\n",
    "    def update(data:Graph[Int,Int]):Unit = {\n",
    "      data.persist()  // 每轮迭代都persist\n",
    "      updateCount += 1\n",
    "      if(updateCount%interval == 0)   // 每隔interval进行checkpoint\n",
    "        data.checkpoint()\n",
    "    }\n",
    "\n",
    "    var g = Graph.fromEdges(sc.parallelize(Array(Edge(1l,3l,1),\n",
    "      Edge(2l,4l,1),\n",
    "      Edge(3l,4l,1))),1)\n",
    "\n",
    "    g.persist()\n",
    "    println(g.vertices.count())\n",
    "\n",
    "    for(i <- 1 to 20){\n",
    "      println(s\"Iteration $i\")\n",
    "      val newGraph = g.mapVertices((vid,vattr) => (vattr*i)/17)\n",
    "      g = g.outerJoinVertices(newGraph.vertices)({(vid,vAttr,newAttr) => newAttr.getOrElse(-99)})\n",
    "      update(g)\n",
    "      println(g.vertices.count)\n",
    "    }\n",
    "\n",
    "    g.triplets.collect.foreach(println)\n",
    "}\n",
    "fun()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
